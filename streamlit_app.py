import streamlit as st
import streamlit.components.v1 as components
import pdfplumber
import pandas as pd
import io
import re
import os
import unicodedata
import traceback
from typing import List, Dict, Any
from openpyxl import load_workbook
from openpyxl.utils.dataframe import dataframe_to_rows

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="ã€æ•°å‡ºè¡¨ã€‘PDF â†’ Excelã¸ã®å¤‰æ›",
    page_icon="./static/favicon.ico",
    layout="centered",
)

# --- Streamlit Session Stateã®åˆæœŸåŒ– ---
if 'master_df' not in st.session_state:
    master_csv_path = "å•†å“ãƒã‚¹ã‚¿ä¸€è¦§.csv"
    initial_master_df = None
    if os.path.exists(master_csv_path):
        encodings = ['utf-8-sig', 'utf-8', 'cp932', 'shift_jis', 'euc-jp', 'iso-2022-jp']
        for encoding in encodings:
            try:
                temp_df = pd.read_csv(master_csv_path, encoding=encoding)
                if not temp_df.empty:
                    initial_master_df = temp_df
                    st.success(f"æ—¢å­˜ã®ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ {encoding} ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
                    break
            except (UnicodeDecodeError, pd.errors.EmptyDataError):
                continue
            except Exception as e:
                st.warning(f"æ—¢å­˜ãƒã‚¹ã‚¿CSV ({master_csv_path}) ã‚’ {encoding} ã§èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                continue
    if initial_master_df is None:
        st.warning(f"ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ '{master_csv_path}' ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹ã€èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚ãƒã‚¹ã‚¿è¨­å®šãƒšãƒ¼ã‚¸ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        initial_master_df = pd.DataFrame(columns=['å•†å“äºˆå®šå', 'ãƒ‘ãƒ³ç®±å…¥æ•°', 'å•†å“å'])
    st.session_state.master_df = initial_master_df

# --- HTML/CSS, ã‚µã‚¤ãƒ‰ãƒãƒ¼ ---
components.html("""<link rel="manifest" href="./static/manifest.json">""", height=0)
st.markdown("""<style>.stApp { background: #fff5e6; }</style>""", unsafe_allow_html=True)
st.sidebar.title("ãƒ¡ãƒ‹ãƒ¥ãƒ¼")
page_selection = st.sidebar.radio("è¡¨ç¤ºã™ã‚‹æ©Ÿèƒ½ã‚’é¸æŠã—ã¦ãã ã•ã„", ("PDF â†’ Excel å¤‰æ›", "ãƒã‚¹ã‚¿è¨­å®š"), index=0)
st.markdown("---")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# âœ…ã€å¤‰æ›´ç‚¹ã€‘æ•°å¼ã‚’ä¿è­·ã—ãªãŒã‚‰æ›¸ãè¾¼ã‚€æ–°ã—ã„ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def safe_write_df(worksheet, df, start_row=2):
    """
    æ•°å¼ã‚’ä¿è­·ã™ã‚‹ãŸã‚ã€æŒ‡å®šã•ã‚ŒãŸç¯„å›²ã®ã‚»ãƒ«ã®ã¿ã‚’ã‚¯ãƒªã‚¢ã—ã€ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æ›¸ãè¾¼ã‚€
    """
    num_cols = df.shape[1]
    
    # 1. æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒªã‚¢ï¼ˆæŒ‡å®šåˆ—ã®ã¿ï¼‰
    # æ›¸ãè¾¼ã‚€è¡Œæ•°ã‚ˆã‚Šæ—¢å­˜ã®è¡Œæ•°ãŒå¤šã„å ´åˆã€ä½™åˆ†ãªè¡Œã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªã‚¢ã™ã‚‹
    if worksheet.max_row >= start_row:
        for row in range(start_row, worksheet.max_row + 1):
            for col in range(1, num_cols + 1):
                worksheet.cell(row=row, column=col).value = None

    # 2. æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã®æ›¸ãè¾¼ã¿ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼ã¯é™¤ãï¼‰
    for r_idx, row_data in enumerate(df.itertuples(index=False), start=start_row):
        for c_idx, value in enumerate(row_data, start=1):
            worksheet.cell(row=r_idx, column=c_idx, value=value)

# (PDFè§£æãƒ»ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºé–¢æ•°ç¾¤ã¯å¤‰æ›´ãŒãªã„ãŸã‚çœç•¥ã—ã¾ã™)
# ... (All the data extraction functions like extract_detailed_client_info_from_pdf, etc. go here) ...
def extract_detailed_client_info_from_pdf(pdf_file_obj):
    """PDFã‹ã‚‰è©³ç´°ãªã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæƒ…å ±ï¼ˆåå‰ï¼‹çµ¦é£Ÿã®æ•°ï¼‰ã‚’æŠ½å‡ºã™ã‚‹"""
    client_data = []

    try:
        with pdfplumber.open(pdf_file_obj) as pdf:
            for page_num, page in enumerate(pdf.pages):
                rows = extract_text_with_layout(page)
                if not rows:
                    continue
                garden_row_idx = -1
                for i, row in enumerate(rows):
                    row_text = ''.join(str(cell) for cell in row if cell)
                    if 'åœ’å' in row_text:
                        garden_row_idx = i
                        break
                if garden_row_idx == -1:
                    continue
                current_client_id = None
                current_client_name = None
                for i in range(garden_row_idx + 1, len(rows)):
                    row = rows[i]
                    row_text = ''.join(str(cell) for cell in row if cell)
                    if '10001' in row_text:
                        break
                    if not any(str(cell).strip() for cell in row):
                        continue
                    if len(row) > 0 and row[0]:
                        left_cell = str(row[0]).strip()
                        if re.match(r'^\d+$', left_cell):
                            if current_client_id and current_client_name:
                                client_info = extract_meal_numbers_from_row(rows, i-1, current_client_id, current_client_name)
                                if client_info:
                                    client_data.append(client_info)
                            current_client_id = left_cell
                            current_client_name = None
                        elif not re.match(r'^\d+$', left_cell) and current_client_id:
                            current_client_name = left_cell
                if current_client_id and current_client_name:
                    client_info = extract_meal_numbers_from_row(rows, len(rows)-1, current_client_id, current_client_name)
                    if client_info:
                        client_data.append(client_info)
    except Exception as e:
        st.error(f"ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæƒ…å ±æŠ½å‡ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    return client_data

def extract_meal_numbers_from_row(rows, row_idx, client_id, client_name):
    client_info = {'client_id': client_id, 'client_name': client_name, 'student_meals': [], 'teacher_meals': []}
    rows_to_check = []
    id_row_idx = -1
    name_row_idx = -1
    for i in range(max(0, row_idx - 3), min(len(rows), row_idx + 3)):
        if i < len(rows) and len(rows[i]) > 0:
            left_cell = str(rows[i][0]).strip()
            if left_cell == client_id:
                id_row_idx = i
                rows_to_check.append(('id', i, rows[i]))
            elif left_cell == client_name:
                name_row_idx = i
                rows_to_check.append(('name', i, rows[i]))
    all_numbers = []
    for row_type, idx, row in rows_to_check:
        for col_idx in range(1, len(row)):
            cell = str(row[col_idx]).strip()
            if cell and re.match(r'^\d+$', cell):
                all_numbers.append({'number': int(cell), 'row_type': row_type, 'col_idx': col_idx})
            elif cell and not re.match(r'^\d+$', cell) and cell != '':
                break
    id_numbers = [item['number'] for item in all_numbers if item['row_type'] == 'id']
    name_numbers = [item['number'] for item in all_numbers if item['row_type'] == 'name']
    client_info['student_meals'] = id_numbers[:3]
    client_info['teacher_meals'] = name_numbers[:2]
    return client_info

def export_detailed_client_data_to_dataframe(client_data):
    df_data = []
    for client_info in client_data:
        row = {
            'ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå': client_info['client_name'],
            'åœ’å…ã®çµ¦é£Ÿã®æ•°1': client_info['student_meals'][0] if len(client_info['student_meals']) > 0 else '',
            'åœ’å…ã®çµ¦é£Ÿã®æ•°2': client_info['student_meals'][1] if len(client_info['student_meals']) > 1 else '',
            'åœ’å…ã®çµ¦é£Ÿã®æ•°3': client_info['student_meals'][2] if len(client_info['student_meals']) > 2 else '',
            'å…ˆç”Ÿã®çµ¦é£Ÿã®æ•°1': client_info['teacher_meals'][0] if len(client_info['teacher_meals']) > 0 else '',
            'å…ˆç”Ÿã®çµ¦é£Ÿã®æ•°2': client_info['teacher_meals'][1] if len(client_info['teacher_meals']) > 1 else '',
        }
        df_data.append(row)
    return pd.DataFrame(df_data)

def pdf_to_excel_data_for_paste_sheet(pdf_file):
    try:
        with pdfplumber.open(pdf_file) as pdf:
            if not pdf.pages:
                st.warning("PDFã«ãƒšãƒ¼ã‚¸ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                return None
            page = pdf.pages[0]
            rows = extract_text_with_layout(page)
            rows = [row for row in rows if any(cell.strip() for cell in row)]
            if not rows:
                st.warning("PDFã®æœ€åˆã®ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                return None
            rows = post_process_rows(rows)
            rows = remove_extra_empty_columns(rows)
            if not rows or not rows[0]:
                st.warning("ç©ºã®åˆ—ã‚’å‰Šé™¤ã—ãŸçµæœã€ãƒ‡ãƒ¼ã‚¿ãŒãªããªã‚Šã¾ã—ãŸã€‚")
                return None
            max_cols = max(len(row) for row in rows) if rows else 0
            normalized_rows = [row + [''] * (max_cols - len(row)) for row in rows]
            df = pd.DataFrame(normalized_rows)
            return df
    except Exception as e:
        st.error(f"PDFå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return None

def extract_table_from_pdf_for_bento(pdf_file_obj):
    tables = []
    with pdfplumber.open(pdf_file_obj) as pdf:
        for page in pdf.pages:
            text = page.extract_text()
            if text is None: continue
            if not any(kw in text for kw in ["åœ’å", "é£¯ã‚ã‚Š", "ã‚­ãƒ£ãƒ©å¼"]):
                continue
            lines = page.lines
            if not lines: continue
            y_coords = sorted(set([line['top'] for line in lines] + [line['bottom'] for line in lines]))
            if len(y_coords) < 2: continue
            table_top = min(y_coords)
            table_bottom = max(y_coords)
            x_coords = sorted(set([line['x0'] for line in lines] + [line['x1'] for line in lines]))
            if len(x_coords) < 2: continue
            table_left = min(x_coords)
            table_right = max(x_coords)
            table_bbox = (table_left, table_top, table_right, table_bottom)
            cropped_page = page.crop(table_bbox)
            table_settings = {"vertical_strategy": "lines", "horizontal_strategy": "lines", "snap_tolerance": 3, "join_tolerance": 3, "edge_min_length": 15}
            table = cropped_page.extract_table(table_settings)
            if table:
                tables.append(table)
    return tables

def find_correct_anchor_for_bento(table, target_row_text="èµ¤"):
    for row_idx, row in enumerate(table):
        row_text = ''.join(str(cell) for cell in row if cell)
        if target_row_text in row_text:
            for offset in [1, 2]:
                if row_idx + offset < len(table):
                    next_row = table[row_idx + offset]
                    for col_idx, cell in enumerate(next_row):
                        if cell and "é£¯ãªã—" in cell:
                            return col_idx
    return -1

def extract_bento_range_for_bento(table, start_col):
    bento_list = []
    end_col = -1
    for row in table:
        row_text = ''.join(str(cell) for cell in row if cell)
        if "ãŠã‚„ã¤" in row_text:
            for col_idx, cell in enumerate(row):
                if cell and "ãŠã‚„ã¤" in cell:
                    end_col = col_idx
                    break
            if end_col != -1:
                break
    if end_col == -1 or start_col >= end_col:
        return []
    header_row_idx = None
    anchor_row_idx = -1
    for row_idx, row in enumerate(table):
        if any(cell and "é£¯ãªã—" in cell for cell in row):
            anchor_row_idx = row_idx
            break
    if anchor_row_idx == -1: return []
    if anchor_row_idx - 1 >= 0:
        header_row_idx = anchor_row_idx - 1
    else:
        return []
    for col in range(start_col + 1, end_col + 1):
        cell_text = table[header_row_idx][col] if col < len(table[header_row_idx]) else ""
        if cell_text and str(cell_text).strip() and "é£¯ãªã—" not in str(cell_text):
            bento_list.append(str(cell_text).strip())
    return bento_list

def match_bento_names(pdf_bento_list, master_df):
    if master_df is None or master_df.empty:
        st.error("ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return [f"{name} (ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ãªã—)" for name in pdf_bento_list]
    master_data_tuples = []
    try:
        if 'å•†å“äºˆå®šå' in master_df.columns and 'ãƒ‘ãƒ³ç®±å…¥æ•°' in master_df.columns:
            master_data_tuples = master_df[['å•†å“äºˆå®šå', 'ãƒ‘ãƒ³ç®±å…¥æ•°']].dropna().values.tolist()
            master_data_tuples = [(str(name), str(value)) for name, value in master_data_tuples]
        else:
            return [f"{name} (å¿…è¦åˆ—ãªã—)" for name in pdf_bento_list]
    except Exception as e:
        st.error(f"ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return [f"{name} (å‡¦ç†ã‚¨ãƒ©ãƒ¼)" for name in pdf_bento_list]

    matched = []
    normalized_master_data_tuples = []
    for master_name, master_id in master_data_tuples:
        normalized_name = unicodedata.normalize('NFKC', master_name)
        normalized_name = re.sub(r'\s+', '', normalized_name)
        normalized_master_data_tuples.append((normalized_name, master_name, master_id))

    for pdf_name in pdf_bento_list:
        original_normalized_pdf_name = unicodedata.normalize('NFKC', str(pdf_name))
        original_normalized_pdf_name = re.sub(r'\s+', '', original_normalized_pdf_name)
        found_match = False
        found_original_master_name = None
        found_id = None
        for norm_m_name, orig_m_name, m_id in normalized_master_data_tuples:
            if norm_m_name.startswith(original_normalized_pdf_name):
                found_original_master_name = orig_m_name
                found_id = m_id
                found_match = True
                break
        if not found_match:
            for norm_m_name, orig_m_name, m_id in normalized_master_data_tuples:
                if original_normalized_pdf_name in norm_m_name:
                    found_original_master_name = orig_m_name
                    found_id = m_id
                    found_match = True
                    break
        if found_original_master_name:
            if found_id:
                matched.append(f"{found_original_master_name} (å…¥æ•°: {found_id})")
            else:
                matched.append(found_original_master_name)
        else:
            matched.append(f"{pdf_name} (æœªãƒãƒƒãƒ)")
    return matched
def extract_text_with_layout(page) -> List[List[str]]:
    words = page.extract_words(x_tolerance=3, y_tolerance=3, keep_blank_chars=False)
    if not words: return []
    boundaries = get_vertical_boundaries(page)
    if len(boundaries) < 2:
        lines = page.extract_text(layout=False, x_tolerance=3, y_tolerance=3)
        return [[line] for line in lines.split('\n') if line.strip()]
    row_groups = get_line_groups(words, y_tolerance=1.5)
    result_rows = []
    for group in row_groups:
        sorted_group = sorted(group, key=lambda w: w['x0'])
        columns = split_line_using_boundaries(sorted_group, boundaries)
        if any(cell.strip() for cell in columns):
            result_rows.append(columns)
    return result_rows
def remove_extra_empty_columns(rows: List[List[str]]) -> List[List[str]]:
    if not rows: return rows
    num_cols = max(len(row) for row in rows) if rows else 0
    if num_cols == 0: return rows
    is_col_empty = [True] * num_cols
    for r, row in enumerate(rows):
        for c in range(len(row)):
            if c < num_cols and row[c].strip():
                is_col_empty[c] = False
    keep_indices = [c for c in range(num_cols) if not is_col_empty[c]]
    new_rows = []
    for row in rows:
        new_row = [row[i] if i < len(row) else "" for i in keep_indices]
        new_rows.append(new_row)
    return new_rows
def post_process_rows(rows: List[List[str]]) -> List[List[str]]:
    new_rows = [row[:] for row in rows]
    for i, row in enumerate(new_rows):
        for j, cell in enumerate(row):
            if "åˆè¨ˆ" in str(cell):
                if i > 0 and j < len(new_rows[i-1]):
                    new_rows[i-1][j] = ""
    return new_rows


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if page_selection == "PDF â†’ Excel å¤‰æ›":
    st.markdown('<div class="title">ã€æ•°å‡ºè¡¨ã€‘PDF â†’ Excelã¸ã®å¤‰æ›</div>', unsafe_allow_html=True)

    uploaded_pdf = st.file_uploader("å‡¦ç†ã™ã‚‹PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„", type="pdf")

    if uploaded_pdf is not None:
        template_path = "template.xlsm"
        nouhinsyo_path = "nouhinsyo.xlsx"
        if not os.path.exists(template_path) or not os.path.exists(nouhinsyo_path):
            st.error(f"'{template_path}' ã¾ãŸã¯ '{nouhinsyo_path}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            st.stop()
        
        # --- å‡¦ç†ã®ãŸã³ã«ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ãƒ¡ãƒ¢ãƒªã«èª­ã¿è¾¼ã‚€ ---
        template_wb = load_workbook(template_path, keep_vba=True)
        nouhinsyo_wb = load_workbook(nouhinsyo_path)

        pdf_bytes_io = io.BytesIO(uploaded_pdf.getvalue())
        
        # --- ãƒ‡ãƒ¼ã‚¿æŠ½å‡º ---
        df_paste_sheet, df_bento_sheet, df_client_sheet = None, None, None
        with st.spinner("PDFã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºä¸­..."):
            df_paste_sheet = pdf_to_excel_data_for_paste_sheet(io.BytesIO(pdf_bytes_io.getvalue()))
            if df_paste_sheet is not None:
                # å¼å½“æŠ½å‡º
                try:
                    tables = extract_table_from_pdf_for_bento(io.BytesIO(pdf_bytes_io.getvalue()))
                    if tables:
                        main_table = max(tables, key=lambda t: len(t) * len(t[0]))
                        if main_table:
                            anchor_col = find_correct_anchor_for_bento(main_table)
                            if anchor_col != -1:
                                bento_list = extract_bento_range_for_bento(main_table, anchor_col)
                                if bento_list:
                                    matched_list = match_bento_names(bento_list, st.session_state.master_df)
                                    output_data_bento = []
                                    for item in matched_list:
                                        match = re.search(r' \(å…¥æ•°: (.+?)\)$', item)
                                        if match:
                                            bento_name, bento_count = item[:match.start()], match.group(1)
                                            output_data_bento.append([bento_name.strip(), bento_count.strip()])
                                        elif "(æœªãƒãƒƒãƒ)" in item:
                                            output_data_bento.append([item.replace(" (æœªãƒãƒƒãƒ)", "").strip(), ""])
                                        else:
                                            output_data_bento.append([item.strip(), ""])
                                    df_bento_sheet = pd.DataFrame(output_data_bento, columns=['å•†å“äºˆå®šå', 'ãƒ‘ãƒ³ç®±å…¥æ•°'])
                except Exception as e:
                    st.error(f"æ³¨æ–‡å¼å½“ãƒ‡ãƒ¼ã‚¿å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                
                # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæŠ½å‡º
                try:
                    client_data = extract_detailed_client_info_from_pdf(io.BytesIO(pdf_bytes_io.getvalue()))
                    if client_data:
                        df_client_sheet = export_detailed_client_data_to_dataframe(client_data)
                        st.success(f"ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæƒ…å ± {len(client_data)} ä»¶ã‚’æŠ½å‡ºã—ã¾ã—ãŸ")
                except Exception as e:
                    st.error(f"ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæƒ…å ±æŠ½å‡ºä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        
        if df_paste_sheet is not None:
            try:
                # --- template.xlsmã¸ã®æ›¸ãè¾¼ã¿ ---
                with st.spinner("template.xlsm ã‚’ä½œæˆä¸­..."):
                    # è²¼ã‚Šä»˜ã‘ç”¨ã‚·ãƒ¼ãƒˆ (ã“ã“ã¯ã‚»ãƒ«æŒ‡å®šãªã®ã§å¾“æ¥é€šã‚Š)
                    ws_paste = template_wb["è²¼ã‚Šä»˜ã‘ç”¨"]
                    for r_idx, row in df_paste_sheet.iterrows():
                        for c_idx, value in enumerate(row):
                            ws_paste.cell(row=r_idx + 1, column=c_idx + 1, value=value)
                    
                    # æ³¨æ–‡å¼å½“ã®æŠ½å‡º (A,Båˆ—ã®ã¿æ›¸ãè¾¼ã¿)
                    if df_bento_sheet is not None:
                        ws_bento = template_wb["æ³¨æ–‡å¼å½“ã®æŠ½å‡º"]
                        safe_write_df(ws_bento, df_bento_sheet, start_row=2)
                    
                    # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæŠ½å‡º (A-Fåˆ—ã®ã¿æ›¸ãè¾¼ã¿)
                    if df_client_sheet is not None:
                        ws_client = template_wb["ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæŠ½å‡º"]
                        safe_write_df(ws_client, df_client_sheet, start_row=2)

                    output_macro = io.BytesIO()
                    template_wb.save(output_macro)
                    macro_excel_bytes = output_macro.getvalue()

                # --- nouhinsyo.xlsxã¸ã®æ›¸ãè¾¼ã¿ ---
                with st.spinner("nouhinsyo.xlsx ã‚’ä½œæˆä¸­..."):
                    df_bento_for_nouhin = None
                    if df_bento_sheet is not None:
                        master_df = st.session_state.master_df
                        master_map = master_df.drop_duplicates(subset=['å•†å“äºˆå®šå']).set_index('å•†å“äºˆå®šå')['å•†å“å'].to_dict()
                        df_bento_for_nouhin = df_bento_sheet.copy()
                        df_bento_for_nouhin['å•†å“å'] = df_bento_for_nouhin['å•†å“äºˆå®šå'].map(master_map)
                        df_bento_for_nouhin = df_bento_for_nouhin[['å•†å“äºˆå®šå', 'ãƒ‘ãƒ³ç®±å…¥æ•°', 'å•†å“å']]

                    # è²¼ã‚Šä»˜ã‘ç”¨ã‚·ãƒ¼ãƒˆ
                    ws_paste_n = nouhinsyo_wb["è²¼ã‚Šä»˜ã‘ç”¨"]
                    for r_idx, row in df_paste_sheet.iterrows():
                        for c_idx, value in enumerate(row):
                            ws_paste_n.cell(row=r_idx + 1, column=c_idx + 1, value=value)
                    
                    # æ³¨æ–‡å¼å½“ã®æŠ½å‡º (A,B,Cåˆ—ã®ã¿æ›¸ãè¾¼ã¿)
                    if df_bento_for_nouhin is not None:
                        ws_bento_n = nouhinsyo_wb["æ³¨æ–‡å¼å½“ã®æŠ½å‡º"]
                        safe_write_df(ws_bento_n, df_bento_for_nouhin, start_row=2)
                    
                    # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæŠ½å‡º
                    if df_client_sheet is not None:
                        ws_client_n = nouhinsyo_wb["ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæŠ½å‡º"]
                        safe_write_df(ws_client_n, df_client_sheet, start_row=2)
                    
                    output_data_only = io.BytesIO()
                    nouhinsyo_wb.save(output_data_only)
                    data_only_excel_bytes = output_data_only.getvalue()

                # --- ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ã®è¡¨ç¤º ---
                st.success("âœ… ãƒ•ã‚¡ã‚¤ãƒ«ã®æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
                original_pdf_name = os.path.splitext(uploaded_pdf.name)[0]
                
                col1, col2 = st.columns(2)
                with col1:
                    st.download_button(
                        label="ğŸ“¥ ãƒã‚¯ãƒ­ä»˜ãExcelã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=macro_excel_bytes,
                        file_name=f"{original_pdf_name}_Processed.xlsm",
                        mime="application/vnd.ms-excel.sheet.macroEnabled.12",
                    )
                with col2:
                    st.download_button(
                        label="ğŸ“¥ ãƒ‡ãƒ¼ã‚¿Excelã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=data_only_excel_bytes,
                        file_name=f"{original_pdf_name}_Data.xlsx",
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    )
            except Exception as e:
                st.error(f"Excelãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                traceback.print_exc()

# ãƒã‚¹ã‚¿è¨­å®š ãƒšãƒ¼ã‚¸ (å¤‰æ›´ãªã—)
elif page_selection == "ãƒã‚¹ã‚¿è¨­å®š":
    # (çœç•¥)
    st.markdown('<div class="title">ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿è¨­å®š</div>', unsafe_allow_html=True)
    master_csv_path = "å•†å“ãƒã‚¹ã‚¿ä¸€è¦§.csv"
    st.markdown("#### æ–°ã—ã„ãƒã‚¹ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    uploaded_master_csv = st.file_uploader(
        "å•†å“ãƒã‚¹ã‚¿ä¸€è¦§.csv ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„",
        type="csv",
        help="ãƒ˜ãƒƒãƒ€ãƒ¼ã«ã¯ 'å•†å“äºˆå®šå', 'ãƒ‘ãƒ³ç®±å…¥æ•°', 'å•†å“å' ã‚’å«ã‚ã¦ãã ã•ã„ã€‚"
    )
    if uploaded_master_csv is not None:
        try:
            new_master_df = None
            encodings = ['utf-8-sig', 'utf-8', 'cp932', 'shift_jis']
            for encoding in encodings:
                try:
                    uploaded_master_csv.seek(0)
                    temp_df = pd.read_csv(uploaded_master_csv, encoding=encoding)
                    if all(col in temp_df.columns for col in ['å•†å“äºˆå®šå', 'ãƒ‘ãƒ³ç®±å…¥æ•°', 'å•†å“å']):
                        new_master_df = temp_df
                        st.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ {encoding} ã§èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
                        break
                    else:
                        st.warning(f"{encoding} ã§èª­ã¿è¾¼ã¿ã¾ã—ãŸãŒã€å¿…é ˆåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                except (UnicodeDecodeError, pd.errors.ParserError):
                    continue
                except Exception as e:
                    st.error(f"èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                    break
            if new_master_df is not None:
                st.session_state.master_df = new_master_df
                try:
                    new_master_df.to_csv(master_csv_path, index=False, encoding='utf-8-sig')
                    st.success(f"âœ… ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°ã—ã€'{master_csv_path}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
                except Exception as e:
                    st.error(f"ãƒã‚¹ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            else:
                st.error("CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ­£ã—ãèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚")
        except Exception as e:
            st.error(f"ãƒã‚¹ã‚¿æ›´æ–°å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
    st.markdown("#### ç¾åœ¨ã®ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿")
    if 'master_df' in st.session_state and not st.session_state.master_df.empty:
        st.dataframe(st.session_state.master_df, use_container_width=True)
    else:
        st.warning("ç¾åœ¨ã€ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
