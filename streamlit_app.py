import streamlit as st
import streamlit.components.v1 as components
import pdfplumber
import pandas as pd
import io
import re
import base64
import os
import unicodedata
import traceback
from typing import List, Dict, Any
from openpyxl import load_workbook

# âœ… ä¿®æ­£: st.set_page_config() ã‚’æœ€åˆã«ç§»å‹•
st.set_page_config(
    page_title="ã€æ•°å‡ºè¡¨ã€‘PDF â†’ Excelã¸ã®å¤‰æ›",
    page_icon="./static/favicon.ico", # faviconã®ãƒ‘ã‚¹ã‚’ä¿®æ­£
    layout="centered",
)

# --- Streamlit Session Stateã®åˆæœŸåŒ– ---
# ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã§ç®¡ç†ã—ã€ã‚¢ãƒ—ãƒªå®Ÿè¡Œä¸­ã«ä¿æŒã™ã‚‹
if 'master_df' not in st.session_state:
    # ã‚¢ãƒ—ãƒªèµ·å‹•æ™‚ã«æ—¢å­˜ã®å•†å“ãƒã‚¹ã‚¿CSVã‚’èª­ã¿è¾¼ã‚€è©¦ã¿
    master_csv_path = "å•†å“ãƒã‚¹ã‚¿ä¸€è¦§.csv"
    initial_master_df = None
    if os.path.exists(master_csv_path):
        # âœ… èª­ã¿è¾¼ã¿ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã« utf-8-sig ã‚’è¿½åŠ 
        encodings = ['utf-8-sig', 'utf-8', 'cp932', 'shift_jis', 'euc-jp', 'iso-2022-jp']
        for encoding in encodings:
            try:
                temp_df = pd.read_csv(master_csv_path, encoding=encoding)
                if not temp_df.empty:
                    initial_master_df = temp_df
                    st.success(f"æ—¢å­˜ã®ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ {encoding} ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
                    break
            except (UnicodeDecodeError, pd.errors.EmptyDataError):
                continue
            except Exception as e:
                st.warning(f"æ—¢å­˜ãƒã‚¹ã‚¿CSV ({master_csv_path}) ã‚’ {encoding} ã§èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                continue
    if initial_master_df is None:
        st.warning(f"ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ '{master_csv_path}' ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹ã€èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚ãƒã‚¹ã‚¿è¨­å®šãƒšãƒ¼ã‚¸ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        initial_master_df = pd.DataFrame(columns=['å•†å“äºˆå®šå', 'ãƒ‘ãƒ³ç®±å…¥æ•°']) # ç©ºã®DataFrameã§åˆæœŸåŒ–
    st.session_state.master_df = initial_master_df

# ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆExcelãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹è¨­å®šã¨å­˜åœ¨ç¢ºèª (ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã§ç®¡ç†)
if 'template_wb_loaded' not in st.session_state:
    st.session_state.template_wb_loaded = False
    st.session_state.template_wb = None

template_path = "template.xlsm"

if not st.session_state.template_wb_loaded:
    if not os.path.exists(template_path):
        st.error(f"ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ« '{template_path}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨åŒã˜å ´æ‰€ã«é…ç½®ã—ã¦ãã ã•ã„ã€‚")
        st.stop()
    
    try:
        st.session_state.template_wb = load_workbook(template_path, keep_vba=True)
        st.session_state.template_wb_loaded = True
        st.success(f"ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ« '{template_path}' ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
    except Exception as e:
        st.error(f"ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ« '{template_path}' ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        st.session_state.template_wb = None
        st.stop()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â‘  HTML <head> åŸ‹ã‚è¾¼ã¿ï¼ˆPWAç”¨ manifest & å„ç¨®ã‚¢ã‚¤ã‚³ãƒ³ï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
components.html(
    """
    <link rel="manifest" href="./static/manifest.json">
    <link rel="icon" href="./static/favicon.ico">
    <link rel="apple-touch-icon" sizes="180x180" href="./static/icons/apple-touch-icon.png">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-title" content="YourAppName">
    """,
    height=0,
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â‘¢ CSSï¼UI ã‚¹ã‚¿ã‚¤ãƒ«å®šç¾©
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.markdown("""
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600&family=Roboto:wght@300;400;500&display=swap');
        .stApp { background: #fff5e6; font-family: 'Inter', sans-serif; }
        .title { font-size: 1.5rem; font-weight: 600; color: #333; margin-bottom: 5px; }
        .subtitle { font-size: 0.9rem; color: #666; margin-bottom: 25px; }
        [data-testid="stFileUploader"] { background: #fff; border-radius: 10px; border: 1px dashed #d0d0d0; padding: 30px 20px; margin: 20px 0; }
        [data-testid="stFileUploader"] label { display: none; }
        [data-testid="stFileUploader"] section { border: none !important; background: transparent !important; }
        .file-card { background: white; border-radius: 8px; padding: 12px 16px; margin: 15px 0; box-shadow: 0 1px 3px rgba(0,0,0,0.08); display: flex; align-items: center; justify-content: space-between; border: 1px solid #eaeaea; }
        .file-info { display: flex; align-items: center; }
        .file-icon { width: 36px; height: 36px; border-radius: 6px; background-color: #f44336; display: flex; align-items: center; justify-content: center; margin-right: 12px; color: white; font-weight: 500; font-size: 14px; }
        .file-details { display: flex; flex-direction: column; }
        .file-name { font-weight: 500; color: #333; font-size: 0.9rem; margin-bottom: 3px; }
        .file-meta { font-size: 0.75rem; color: #888; }
        .loading-spinner { width: 20px; height: 20px; border: 2px solid rgba(0,0,0,0.1); border-radius: 50%; border-top-color: #ff9933; animation: spin 1s linear infinite; }
        .check-icon { color: #ff9933; font-size: 20px; }
        @keyframes spin { to { transform: rotate(360deg); } }
        .progress-bar { height: 4px; background-color: #e0e0e0; border-radius: 2px; width: 100%; margin-top: 10px; overflow: hidden; }
        .progress-value { height: 100%; background-color: #ff9933; border-radius: 2px; width: 60%; transition: width 0.5s ease-in-out; }
        .download-card { background: white; border-radius: 8px; padding: 16px; margin: 20px 0; box-shadow: 0 2px 5px rgba(0,0,0,0.08); display: flex; align-items: center; justify-content: space-between; border: 1px solid #eaeaea; transition: all 0.2s ease; cursor: pointer; text-decoration: none; color: inherit; }
        .download-card:hover { box-shadow: 0 4px 12px rgba(0,0,0,0.12); background-color: #fffaf0; transform: translateY(-2px); }
        .download-info { display: flex; align-items: center; }
        .download-icon { width: 40px; height: 40px; border-radius: 8px; background-color: #ff9933; display: flex; align-items: center; justify-content: center; margin-right: 12px; color: white; font-weight: 500; font-size: 16px; }
        .download-details { display: flex; flex-direction: column; }
        .download-name { font-weight: 500; color: #333; font-size: 0.9rem; margin-bottom: 3px; }
        .download-meta { font-size: 0.75rem; color: #888; }
        .download-button-imitation { background-color: #ff9933; color: white; border: none; border-radius: 6px; padding: 8px 16px; font-size: 0.85rem; font-weight: 500; transition: background-color 0.2s; display: flex; align-items: center; }
        .download-card:hover .download-button-imitation { background-color: #e68a00; }
        .download-button-icon { margin-right: 6px; }
        .separator { height: 1px; background-color: #ffe0b3; margin: 25px 0; }
    </style>
""", unsafe_allow_html=True)


# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ ---
st.sidebar.title("ãƒ¡ãƒ‹ãƒ¥ãƒ¼")
page_selection = st.sidebar.radio(
    "è¡¨ç¤ºã™ã‚‹æ©Ÿèƒ½ã‚’é¸æŠã—ã¦ãã ã•ã„",
    ("PDF â†’ Excel å¤‰æ›", "ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæƒ…å ±æŠ½å‡º", "ãƒã‚¹ã‚¿è¨­å®š"),
    index=0 # åˆæœŸè¡¨ç¤ºã¯ã€ŒPDF â†’ Excel å¤‰æ›ã€
)

st.markdown("---") # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¨ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®åŒºåˆ‡ã‚Š


# --- ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è¡¨ç¤ºãƒ­ã‚¸ãƒƒã‚¯ ---

# ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒŠé–‹å§‹
st.markdown('<div class="main-container">', unsafe_allow_html=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæƒ…å ±æŠ½å‡ºã®æ–°ã—ã„é–¢æ•°ç¾¤
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def extract_client_and_meal_info_from_pdf(pdf_file_obj):
    """PDFã‹ã‚‰åœ’åã®ä¸‹ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåã¨çµ¦é£Ÿã®æ•°ã‚’æŠ½å‡ºã™ã‚‹"""
    client_data = []
    
    try:
        with pdfplumber.open(pdf_file_obj) as pdf:
            for page_num, page in enumerate(pdf.pages):
                st.write(f"ğŸ“„ ãƒšãƒ¼ã‚¸ {page_num + 1} ã‚’å‡¦ç†ä¸­...")
                
                # ç¸¦ç·šã®ä½ç½®ã‚’æ¤œå‡º
                vertical_lines = []
                if page.lines:
                    for line in page.lines:
                        # ç¸¦ç·šã‚’æ¤œå‡ºï¼ˆx0ã¨x1ãŒã»ã¼åŒã˜ã§ã€y0ã¨y1ãŒç•°ãªã‚‹ï¼‰
                        if abs(line['x0'] - line['x1']) < 2:  # ç¸¦ç·šã®åˆ¤å®š
                            vertical_lines.append(line['x0'])
                
                # ç¸¦ç·šã‚’ã‚½ãƒ¼ãƒˆ
                vertical_lines = sorted(list(set(vertical_lines)))
                st.write(f"ğŸ“ æ¤œå‡ºã•ã‚ŒãŸç¸¦ç·šã®ä½ç½®: {vertical_lines}")
                
                # ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
                text = page.extract_text()
                if not text:
                    st.write("âŒ ã“ã®ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
                    continue
                
                # ãƒ‡ãƒãƒƒã‚°ï¼šæŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®ä¸€éƒ¨ã‚’è¡¨ç¤º
                st.write(f"ğŸ” æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®æœ€åˆã®300æ–‡å­—:")
                st.code(text[:300])
                
                lines = text.split('\n')
                st.write(f"ğŸ“ ç·è¡Œæ•°: {len(lines)}")
                
                # åœ’åã‚’æ¢ã™
                garden_found = False
                start_index = -1
                for i, line in enumerate(lines):
                    if 'åœ’å' in line:
                        garden_found = True
                        start_index = i + 1
                        st.write(f"âœ… åœ’åã‚’ç™ºè¦‹ï¼ è¡Œç•ªå·: {i}, å†…å®¹: '{line}'")
                        break
                
                if not garden_found:
                    st.write("âŒ åœ’åãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                    continue
                
                # åœ’åã®ä¸‹ã‹ã‚‰ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåã¨çµ¦é£Ÿæƒ…å ±ã‚’æŠ½å‡º
                st.write(f"ğŸ” åœ’åã®ä¸‹ã®è¡Œã‹ã‚‰æŠ½å‡ºé–‹å§‹ï¼ˆè¡Œ {start_index} ã‹ã‚‰ï¼‰:")
                
                i = start_index
                while i < len(lines):
                    line = lines[i].strip()
                    
                    # 10000ãŒå‡ºã¦ããŸã‚‰çµ‚äº†
                    if '10000' in line:
                        st.write(f"ğŸ›‘ 10000ã‚’ç™ºè¦‹ï¼ å‡¦ç†çµ‚äº†")
                        break
                    
                    # ç©ºè¡Œã¯ã‚¹ã‚­ãƒƒãƒ—
                    if not line:
                        i += 1
                        continue
                    
                    # IDã§å§‹ã¾ã‚‹è¡Œã‚’æ¤œå‡º
                    if re.match(r'^\d+', line):
                        client_info = extract_client_info_from_lines(lines, i)
                        if client_info:
                            client_data.append(client_info)
                            st.write(f"âœ… æŠ½å‡ºå®Œäº†: {client_info}")
                        i += client_info.get('lines_processed', 1)
                    else:
                        i += 1
    
    except Exception as e:
        st.error(f"ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæƒ…å ±æŠ½å‡ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        st.exception(e)
    
    # çµæœã®è¡¨ç¤º
    st.write(f"ğŸ¯ æœ€çµ‚çµæœ: ç· {len(client_data)} ä»¶ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæƒ…å ±ã‚’æŠ½å‡º")
    if client_data:
        st.write("æŠ½å‡ºã•ã‚ŒãŸã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæƒ…å ±:")
        for i, data in enumerate(client_data[:5]):  # æœ€åˆã®5ä»¶ã‚’è¡¨ç¤º
            st.write(f"  {i+1}: {format_client_info(data)}")
        if len(client_data) > 5:
            st.write(f"  ... ä»– {len(client_data) - 5} ä»¶")
    
    return client_data


def extract_client_info_from_lines(lines, start_idx):
    """æŒ‡å®šã•ã‚ŒãŸè¡Œã‹ã‚‰1ã¤ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæƒ…å ±ã‚’æŠ½å‡º"""
    client_info = {
        'client_name': '',
        'student_meals': [],
        'teacher_meals': [],
        'lines_processed': 1
    }
    
    current_line = lines[start_idx].strip()
    
    # IDã‚’æŠ½å‡º
    id_match = re.match(r'^(\d+)', current_line)
    if not id_match:
        return None
    
    client_id = id_match.group(1)
    remaining_text = current_line[len(client_id):].strip()
    
    # åŒã˜è¡Œã«ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    if remaining_text and not remaining_text.isdigit():
        # åŒã˜è¡Œã«ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåãŒã‚ã‚‹å ´åˆ
        client_info['client_name'] = extract_client_name_from_text(remaining_text)
        # æ®‹ã‚Šã®éƒ¨åˆ†ã‹ã‚‰åœ’å…ã®çµ¦é£Ÿã®æ•°ã‚’æŠ½å‡º
        client_info['student_meals'] = extract_numbers_from_text(remaining_text, after_name=True)
        client_info['lines_processed'] = 1
    else:
        # æ¬¡ã®è¡Œã«ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåãŒã‚ã‚‹å ´åˆ
        if start_idx + 1 < len(lines):
            next_line = lines[start_idx + 1].strip()
            if next_line and not next_line.isdigit():
                client_info['client_name'] = extract_client_name_from_text(next_line)
                client_info['teacher_meals'] = extract_numbers_from_text(next_line, after_name=True)
                client_info['lines_processed'] = 2
        
        # IDã®è¡Œã‹ã‚‰åœ’å…ã®çµ¦é£Ÿã®æ•°ã‚’æŠ½å‡º
        if remaining_text:
            client_info['student_meals'] = extract_numbers_from_text(remaining_text)
    
    # è¿½åŠ ã®è¡Œã‚‚ãƒã‚§ãƒƒã‚¯ï¼ˆåœ’å…ã®çµ¦é£Ÿã®æ•°ã‚„å…ˆç”Ÿã®çµ¦é£Ÿã®æ•°ã®ãŸã‚ï¼‰
    check_lines = 3  # æœ€å¤§3è¡Œå…ˆã¾ã§ç¢ºèª
    for offset in range(1, min(check_lines + 1, len(lines) - start_idx)):
        check_line = lines[start_idx + offset].strip()
        
        # æ•°å­—ä»¥å¤–ã®æ–‡å­—ãŒå‡ºã¦ããŸã‚‰çµ‚äº†
        if check_line and not check_line.replace(' ', '').isdigit() and not has_numbers(check_line):
            break
        
        # æ•°å­—ãŒå«ã¾ã‚Œã‚‹è¡Œã‹ã‚‰è¿½åŠ ã®çµ¦é£Ÿã®æ•°ã‚’æŠ½å‡º
        if has_numbers(check_line):
            additional_numbers = extract_numbers_from_text(check_line)
            
            # åœ’å…ã®çµ¦é£Ÿã®æ•°ãŒè¶³ã‚Šãªã„å ´åˆã¯åœ’å…ã«è¿½åŠ 
            if len(client_info['student_meals']) < 3:
                needed = 3 - len(client_info['student_meals'])
                client_info['student_meals'].extend(additional_numbers[:needed])
                additional_numbers = additional_numbers[needed:]
            
            # æ®‹ã‚Šã¯å…ˆç”Ÿã®çµ¦é£Ÿã®æ•°ã«è¿½åŠ 
            client_info['teacher_meals'].extend(additional_numbers)
            
            if offset >= client_info['lines_processed']:
                client_info['lines_processed'] = offset + 1
    
    return client_info


def extract_client_name_from_text(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåã‚’æŠ½å‡ºï¼ˆæ•°å­—ä»¥å¤–ã®éƒ¨åˆ†ï¼‰"""
    # æ•°å­—ã‚’é™¤å»ã—ã¦ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåã‚’æŠ½å‡º
    name_parts = []
    words = text.split()
    
    for word in words:
        if not word.isdigit():
            name_parts.append(word)
        else:
            # æ•°å­—ãŒå‡ºã¦ããŸã‚‰åå‰ã®éƒ¨åˆ†ã¯çµ‚äº†
            break
    
    return ' '.join(name_parts).strip()


def extract_numbers_from_text(text, after_name=False):
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ•°å­—ã‚’æŠ½å‡º"""
    numbers = []
    
    if after_name:
        # åå‰ã®å¾Œã®æ•°å­—ã‚’æŠ½å‡º
        words = text.split()
        name_ended = False
        
        for word in words:
            if word.isdigit():
                numbers.append(int(word))
                name_ended = True
            elif name_ended:
                # åå‰ãŒçµ‚ã‚ã£ãŸå¾Œã«æ•°å­—ä»¥å¤–ãŒå‡ºã¦ããŸã‚‰çµ‚äº†
                break
    else:
        # å˜ç´”ã«æ•°å­—ã‚’æŠ½å‡º
        words = text.split()
        for word in words:
            if word.isdigit():
                numbers.append(int(word))
    
    return numbers


def has_numbers(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã«æ•°å­—ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
    return any(char.isdigit() for char in text)


def format_client_info(client_info):
    """ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæƒ…å ±ã‚’è¦‹ã‚„ã™ã„å½¢å¼ã§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    name = client_info['client_name']
    student_meals = client_info['student_meals']
    teacher_meals = client_info['teacher_meals']
    
    # åœ’å…ã®çµ¦é£Ÿã®æ•°ã‚’3ã¤ã¾ã§è¡¨ç¤ºï¼ˆè¶³ã‚Šãªã„å ´åˆã¯ç©ºç™½ï¼‰
    student_meal_str = []
    for i in range(3):
        if i < len(student_meals):
            student_meal_str.append(str(student_meals[i]))
        else:
            student_meal_str.append('')
    
    # å…ˆç”Ÿã®çµ¦é£Ÿã®æ•°ã‚’2ã¤ã¾ã§è¡¨ç¤ºï¼ˆè¶³ã‚Šãªã„å ´åˆã¯ç©ºç™½ï¼‰
    teacher_meal_str = []
    for i in range(2):
        if i < len(teacher_meals):
            teacher_meal_str.append(str(teacher_meals[i]))
        else:
            teacher_meal_str.append('')
    
    return f"{name}\t{student_meal_str[0]}\t{student_meal_str[1]}\t{student_meal_str[2]}\t{teacher_meal_str[0]}\t{teacher_meal_str[1]}"


def export_client_data_to_csv(client_data, filename="client_meal_data.csv"):
    """ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæƒ…å ±ã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›"""
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ç”¨ã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ
    df_data = []
    
    for client_info in client_data:
        row = {
            'ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå': client_info['client_name'],
            'åœ’å…ã®çµ¦é£Ÿã®æ•°1': client_info['student_meals'][0] if len(client_info['student_meals']) > 0 else '',
            'åœ’å…ã®çµ¦é£Ÿã®æ•°2': client_info['student_meals'][1] if len(client_info['student_meals']) > 1 else '',
            'åœ’å…ã®çµ¦é£Ÿã®æ•°3': client_info['student_meals'][2] if len(client_info['student_meals']) > 2 else '',
            'å…ˆç”Ÿã®çµ¦é£Ÿã®æ•°1': client_info['teacher_meals'][0] if len(client_info['teacher_meals']) > 0 else '',
            'å…ˆç”Ÿã®çµ¦é£Ÿã®æ•°2': client_info['teacher_meals'][1] if len(client_info['teacher_meals']) > 1 else '',
        }
        df_data.append(row)
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆã—ã¦CSVã«å‡ºåŠ›
    df = pd.DataFrame(df_data)
    
    return df


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# æ—¢å­˜ã®PDFâ†’Excelå¤‰æ›ãƒ­ã‚¸ãƒƒã‚¯
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def is_number(text: str) -> bool:
    return bool(re.match(r'^\d+$', text.strip()))

def get_line_groups(words: List[Dict[str, Any]], y_tolerance: float = 1.2) -> List[List[Dict[str, Any]]]:
    if not words:
        return []
    sorted_words = sorted(words, key=lambda w: w['top'])
    groups = []
    current_group = [sorted_words[0]]
    current_top = sorted_words[0]['top']
    for word in sorted_words[1:]:
        if abs(word['top'] - current_top) <= y_tolerance:
            current_group.append(word)
        else:
            groups.append(current_group)
            current_group = [word]
            current_top = word['top']
    groups.append(current_group)
    return groups

def get_vertical_boundaries(page, tolerance: float = 2) -> List[float]:
    vertical_lines_x = []
    for line in page.lines:
        if abs(line['x0'] - line['x1']) < tolerance:
            vertical_lines_x.append((line['x0'] + line['x1']) / 2)
    vertical_lines_x = sorted(list(set(round(x, 1) for x in vertical_lines_x)))

    words = page.extract_words()
    if not words:
        return vertical_lines_x

    left_boundary = min(word['x0'] for word in words)
    right_boundary = max(word['x1'] for word in words)

    boundaries = sorted(list(set([round(left_boundary, 1)] + vertical_lines_x + [round(right_boundary, 1)])))

    merged_boundaries = []
    if boundaries:
        merged_boundaries.append(boundaries[0])
        for i in range(1, len(boundaries)):
            if boundaries[i] - merged_boundaries[-1] > tolerance * 2:
                merged_boundaries.append(boundaries[i])
        if right_boundary > merged_boundaries[-1] + tolerance * 2 :
                merged_boundaries.append(round(right_boundary, 1))
        boundaries = sorted(list(set(merged_boundaries)))

    return boundaries

def split_line_using_boundaries(sorted_words_in_line: List[Dict[str, Any]], boundaries: List[float]) -> List[str]:
    columns = [""] * (len(boundaries) - 1)
    for word in sorted_words_in_line:
        word_center_x = (word['x0'] + word['x1']) / 2
        for i in range(len(boundaries) - 1):
            left = boundaries[i]
            right = boundaries[i + 1]
            if left <= word_center_x < right:
                if columns[i]:
                    columns[i] += " " + word["text"]
                else:
                    columns[i] = word["text"]
                break
    return columns

def extract_text_with_layout(page) -> List[List[str]]:
    words = page.extract_words(x_tolerance=3, y_tolerance=3, keep_blank_chars=False)
    if not words:
        return []

    boundaries = get_vertical_boundaries(page)
    if len(boundaries) < 2:
            lines = page.extract_text(layout=False, x_tolerance=3, y_tolerance=3)
            return [[line] for line in lines.split('\n') if line.strip()]

    row_groups = get_line_groups(words, y_tolerance=1.5)

    result_rows = []
    for group in row_groups:
        sorted_group = sorted(group, key=lambda w: w['x0'])
        columns = split_line_using_boundaries(sorted_group, boundaries)
        if any(cell.strip() for cell in columns):
            result_rows.append(columns)

    return result_rows

def remove_extra_empty_columns(rows: List[List[str]]) -> List[List[str]]:
    if not rows:
        return rows
    num_cols = max(len(row) for row in rows) if rows else 0
    if num_cols == 0:
        return rows

    is_col_empty = [True] * num_cols
    for r, row in enumerate(rows):
        for c in range(len(row)):
            if c < num_cols and row[c].strip():
                is_col_empty[c] = False

    keep_indices = [c for c in range(num_cols) if not is_col_empty[c]]

    new_rows = []
    for row in rows:
        new_row = [row[i] if i < len(row) else "" for i in keep_indices]
        new_rows.append(new_row)

    return new_rows

def post_process_rows(rows: List[List[str]]) -> List[List[str]]:
    new_rows = [row[:] for row in rows]
    for i, row in enumerate(new_rows):
        for j, cell in enumerate(row):
            if "åˆè¨ˆ" in str(cell):
                if i > 0 and j < len(new_rows[i-1]):
                    new_rows[i-1][j] = ""
    return new_rows

def pdf_to_excel_data_for_paste_sheet(pdf_file) -> pd.DataFrame | None:
    try:
        with pdfplumber.open(pdf_file) as pdf:
            if not pdf.pages:
                st.warning("PDFã«ãƒšãƒ¼ã‚¸ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                return None
            page = pdf.pages[0]

            rows = extract_text_with_layout(page)
            rows = [row for row in rows if any(cell.strip() for cell in row)]
            if not rows:
                st.warning("PDFã®æœ€åˆã®ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ï¼ˆè²¼ã‚Šä»˜ã‘ç”¨ï¼‰")
                return None

            rows = post_process_rows(rows)
            rows = remove_extra_empty_columns(rows)
            if not rows or not rows[0]:
                    st.warning("ç©ºã®åˆ—ã‚’å‰Šé™¤ã—ãŸçµæœã€ãƒ‡ãƒ¼ã‚¿ãŒãªããªã‚Šã¾ã—ãŸã€‚ï¼ˆè²¼ã‚Šä»˜ã‘ç”¨ï¼‰")
                    return None

            max_cols = max(len(row) for row in rows) if rows else 0
            normalized_rows = [row + [''] * (max_cols - len(row)) for row in rows]
            df = pd.DataFrame(normalized_rows)
            return df

    except Exception as e:
        st.error(f"PDFå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸï¼ˆè²¼ã‚Šä»˜ã‘ç”¨ï¼‰: {e}")
        return None

def extract_table_from_pdf_for_bento(pdf_file_obj):
    tables = []
    with pdfplumber.open(pdf_file_obj) as pdf:
        for page in pdf.pages:
            text = page.extract_text()
            
            start_keywords = ["åœ’å", "é£¯ã‚ã‚Š", "ã‚­ãƒ£ãƒ©å¼"]
            end_keywords = ["ãŠã‚„ã¤", "åˆè¨ˆ", "PAGE"]
            
            if not any(kw in text for kw in start_keywords):
                continue
                
            lines = page.lines
            if not lines:
                continue
                
            y_coords = sorted(set([line['top'] for line in lines] + [line['bottom'] for line in lines]))
            if len(y_coords) < 2:
                continue
                
            table_top = min(y_coords)
            table_bottom = max(y_coords)
            
            x_coords = sorted(set([line['x0'] for line in lines] + [line['x1'] for line in lines]))
            if len(x_coords) < 2:
                continue
                
            table_left = min(x_coords)
            table_right = max(x_coords)
            
            table_bbox = (table_left, table_top, table_right, table_bottom)
            cropped_page = page.crop(table_bbox)
            
            table_settings = {
                "vertical_strategy": "lines",
                "horizontal_strategy": "lines",
                "snap_tolerance": 3,
                "join_tolerance": 3,
                "edge_min_length": 15,
            }
            
            table = cropped_page.extract_table(table_settings)
            if table:
                tables.append(table)
    
    return tables

def find_correct_anchor_for_bento(table, target_row_text="èµ¤"):
    for row_idx, row in enumerate(table):
        row_text = ''.join(str(cell) for cell in row if cell)
        if target_row_text in row_text:
            for offset in [1, 2]:
                if row_idx + offset < len(table):
                    next_row = table[row_idx + offset]
                    for col_idx, cell in enumerate(next_row):
                        if cell and "é£¯ãªã—" in cell:
                            return col_idx
    return -1

def extract_bento_range_for_bento(table, start_col):
    bento_list = []
    end_col = -1
    
    for row in table:
        row_text = ''.join(str(cell) for cell in row if cell)
        if "ãŠã‚„ã¤" in row_text:
            for col_idx, cell in enumerate(row):
                if cell and "ãŠã‚„ã¤" in cell:
                    end_col = col_idx
                    break
            if end_col != -1:
                break
    
    if end_col == -1 or start_col >= end_col:
        return []
    
    header_row_idx = None
    anchor_row_idx = -1
    for row_idx, row in enumerate(table):
        if any(cell and "é£¯ãªã—" in cell for cell in row):
            anchor_row_idx = row_idx
            break
    
    if anchor_row_idx == -1:
        return []
    
    if anchor_row_idx - 1 >= 0:
        header_row_idx = anchor_row_idx - 1
    else:
        return []
    
    for col in range(start_col + 1, end_col + 1):
        if col < len(table[header_row_idx]):
            cell_text = table[header_row_idx][col]
        else:
            cell_text = ""
        
        if cell_text and str(cell_text).strip() and "é£¯ãªã—" not in str(cell_text):
            bento_list.append(str(cell_text).strip())
    
    return bento_list

def match_bento_names(pdf_bento_list, master_df):
    if master_df is None or master_df.empty:
        st.error("ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ãƒã‚¹ã‚¿è¨­å®šãƒšãƒ¼ã‚¸ã§CSVã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        return [f"{name} (ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ãªã—)" for name in pdf_bento_list]

    master_data_tuples = []
    try:
        if 'å•†å“äºˆå®šå' in master_df.columns and 'ãƒ‘ãƒ³ç®±å…¥æ•°' in master_df.columns:
            master_data_tuples = master_df[['å•†å“äºˆå®šå', 'ãƒ‘ãƒ³ç®±å…¥æ•°']].dropna().values.tolist()
            master_data_tuples = [(str(name), str(value)) for name, value in master_data_tuples]
        elif 'å•†å“äºˆå®šå' in master_df.columns:
            st.warning("è­¦å‘Š: ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ã«ã€Œãƒ‘ãƒ³ç®±å…¥æ•°ã€åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å•†å“äºˆå®šåã®ã¿ã§ç…§åˆã—ã¾ã™ã€‚")
            master_data_tuples = master_df['å•†å“äºˆå®šå'].dropna().astype(str).tolist()
            master_data_tuples = [(name, "") for name in master_data_tuples]
        else:
            st.error("ã‚¨ãƒ©ãƒ¼: ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ã«ã€Œå•†å“äºˆå®šåã€åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return [f"{name} (å•†å“äºˆå®šååˆ—ãªã—)" for name in pdf_bento_list]

    except KeyError as e:
        st.error(f"ã‚¨ãƒ©ãƒ¼: ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ã«å¿…è¦ãªåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}ã€‚CSVã®ãƒ˜ãƒƒãƒ€ãƒ¼åã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return [f"{name} (åˆ—ã‚¨ãƒ©ãƒ¼)" for name in pdf_bento_list]
    except Exception as e:
        st.error(f"ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿å‡¦ç†ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return [f"{name} (å‡¦ç†ã‚¨ãƒ©ãƒ¼)" for name in pdf_bento_list]
    
    if len(master_data_tuples) == 0:
        st.warning("ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æœ‰åŠ¹ãªå•†å“æƒ…å ±ãŒæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        return [f"{name} (ãƒã‚¹ã‚¿ç©º)" for name in pdf_bento_list]

    matched = []
    
    normalized_master_data_tuples = []
    for master_name, master_id in master_data_tuples:
        normalized_name = unicodedata.normalize('NFKC', master_name)
        normalized_name = re.sub(r'\s+', '', normalized_name)
        normalized_master_data_tuples.append((normalized_name, master_name, master_id))
    
    for pdf_name in pdf_bento_list:
        original_normalized_pdf_name = unicodedata.normalize('NFKC', str(pdf_name))
        original_normalized_pdf_name = re.sub(r'\s+', '', original_normalized_pdf_name)
        
        current_pdf_name_for_matching = original_normalized_pdf_name
        
        found_match = False
        found_original_master_name = None
        found_id = None
        
        for norm_m_name, orig_m_name, m_id in normalized_master_data_tuples:
            if norm_m_name.startswith(current_pdf_name_for_matching):
                found_original_master_name = orig_m_name
                found_id = m_id
                found_match = True
                break
        
        if not found_match:
            for norm_m_name, orig_m_name, m_id in normalized_master_data_tuples:
                if current_pdf_name_for_matching in norm_m_name:
                    found_original_master_name = orig_m_name
                    found_id = m_id
                    found_match = True
                    break
        
        if not found_match:
            for num_chars_to_remove in range(1, 4):  
                if len(original_normalized_pdf_name) > num_chars_to_remove:
                    truncated_pdf_name = original_normalized_pdf_name[:-num_chars_to_remove]
                    
                    for norm_m_name, orig_m_name, m_id in normalized_master_data_tuples:
                        if norm_m_name.startswith(truncated_pdf_name):
                            found_original_master_name = orig_m_name
                            found_id = m_id
                            found_match = True
                            break
                    
                    if not found_match:
                        for norm_m_name, orig_m_name, m_id in normalized_master_data_tuples:
                            if truncated_pdf_name in norm_m_name:
                                found_original_master_name = orig_m_name
                                found_id = m_id
                                found_match = True
                                break
                    
                    if found_match:
                        break
        
        if found_original_master_name:
            if found_id:
                matched.append(f"{found_original_master_name} (å…¥æ•°: {found_id})")
            else:
                matched.append(found_original_master_name)
        else:
            matched.append(f"{pdf_name} (æœªãƒãƒƒãƒ)")
    
    return matched

def extract_client_names_from_pdf(pdf_file_obj):
    """PDFã‹ã‚‰åœ’åã®ä¸‹ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåã‚’æŠ½å‡ºã™ã‚‹ï¼ˆç¸¦ç·šã‚’è€ƒæ…®ã—ãŸæ”¹è‰¯ç‰ˆï¼‰"""
    client_names = []

    try:
        with pdfplumber.open(pdf_file_obj) as pdf:
            for page_num, page in enumerate(pdf.pages):
                st.write(f"ğŸ“„ ãƒšãƒ¼ã‚¸ {page_num + 1} ã‚’å‡¦ç†ä¸­...")
            
                # ç¸¦ç·šã®ä½ç½®ã‚’æ¤œå‡º
                vertical_lines = []
                if page.lines:
                    for line in page.lines:
                        # ç¸¦ç·šã‚’æ¤œå‡ºï¼ˆx0ã¨x1ãŒã»ã¼åŒã˜ã§ã€y0ã¨y1ãŒç•°ãªã‚‹ï¼‰
                        if abs(line['x0'] - line['x1']) < 2:  # ç¸¦ç·šã®åˆ¤å®š
                            vertical_lines.append(line['x0'])
            
                # ç¸¦ç·šã‚’ã‚½ãƒ¼ãƒˆ
                vertical_lines = sorted(list(set(vertical_lines)))
                st.write(f"ğŸ“ æ¤œå‡ºã•ã‚ŒãŸç¸¦ç·šã®ä½ç½®: {vertical_lines}")
            
                # 1ã¤ç›®ã¨2ã¤ç›®ã®ç¸¦ç·šã®é–“ã®ç¯„å›²ã‚’è¨­å®š
                if len(vertical_lines) >= 2:
                    left_bound = vertical_lines[0]
                    right_bound = vertical_lines[1]
                    st.write(f"ğŸ¯ ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåæŠ½å‡ºç¯„å›²: x={left_bound:.1f} ã‹ã‚‰ x={right_bound:.1f}")
                else:
                    st.write("âš ï¸ ååˆ†ãªç¸¦ç·šãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã‹ã‚‰æŠ½å‡ºã—ã¾ã™ã€‚")
                    left_bound = None
                    right_bound = None
            
                # ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºï¼ˆç¸¦ç·šã®ç¯„å›²ã‚’è€ƒæ…®ï¼‰
                if left_bound is not None and right_bound is not None:
                    # æŒ‡å®šã•ã‚ŒãŸç¯„å›²å†…ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
                    crop_box = (left_bound, 0, right_bound, page.height)
                    cropped_page = page.crop(crop_box)
                    text = cropped_page.extract_text()
                    st.write(f"ğŸ“„ ç¯„å›²æŒ‡å®šã§ã®ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå®Œäº†")
                else:
                    # é€šå¸¸ã®ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
                    text = page.extract_text()
            
                if not text:
                    st.write("âŒ ã“ã®ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
                    continue
            
                # ãƒ‡ãƒãƒƒã‚°ï¼šæŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®ä¸€éƒ¨ã‚’è¡¨ç¤º
                st.write(f"ğŸ” æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®æœ€åˆã®300æ–‡å­—:")
                st.code(text[:300])
            
                lines = text.split('\n')
                st.write(f"ğŸ“ ç·è¡Œæ•°: {len(lines)}")
            
                # åœ’åã‚’æ¢ã™
                garden_found = False
                start_index = -1
                for i, line in enumerate(lines):
                    if 'åœ’å' in line:
                        garden_found = True
                        start_index = i + 1
                        st.write(f"âœ… åœ’åã‚’ç™ºè¦‹ï¼ è¡Œç•ªå·: {i}, å†…å®¹: '{line}'")
                        break
            
                if not garden_found:
                    st.write("âŒ åœ’åãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                    continue
            
                # åœ’åã®ä¸‹ã‹ã‚‰ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåã‚’æŠ½å‡º
                st.write(f"ğŸ” åœ’åã®ä¸‹ã®è¡Œã‹ã‚‰æŠ½å‡ºé–‹å§‹ï¼ˆè¡Œ {start_index} ã‹ã‚‰ï¼‰:")
            
                extracted_count = 0
                expect_id = True  # æœ€åˆã¯IDã‚’æœŸå¾…
            
                for j in range(start_index, len(lines)):
                    line = lines[j].strip()
                
                    # 10000ãŒå‡ºã¦ããŸã‚‰çµ‚äº†
                    if '10000' in line:
                        st.write(f"ğŸ›‘ 10000ã‚’ç™ºè¦‹ï¼ å‡¦ç†çµ‚äº†")
                        break
                
                    # ç©ºè¡Œã¯ã‚¹ã‚­ãƒƒãƒ—
                    if not line:
                        continue
                
                    # IDã¨ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåã®äº¤äº’ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å‡¦ç†
                    if expect_id:
                        # IDã‚’æœŸå¾…ã—ã¦ã„ã‚‹å ´åˆ
                        if re.match(r'^\d+', line):
                            # IDãŒè¦‹ã¤ã‹ã£ãŸ
                            id_match = re.match(r'^(\d+)', line)
                            if id_match:
                                current_id = id_match.group(1)
                                st.write(f"ğŸ”¢ IDç™ºè¦‹: '{current_id}'")
                                expect_id = False  # æ¬¡ã¯ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåã‚’æœŸå¾…
                            
                                # åŒã˜è¡Œã«ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                                remaining_text = line[len(current_id):].strip()
                                if remaining_text and not remaining_text.isdigit():
                                    # åŒã˜è¡Œã«ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåãŒã‚ã‚‹
                                    client_names.append(remaining_text)
                                    extracted_count += 1
                                    st.write(f"âœ… æŠ½å‡ºï¼ˆID+åå‰ï¼‰: '{remaining_text}'")
                                    expect_id = True  # æ¬¡ã¯ã¾ãŸIDã‚’æœŸå¾…
                        else:
                            # IDãŒæœŸå¾…ã•ã‚Œã¦ã„ã‚‹ã®ã«è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                            st.write(f"â­ï¸ IDæœŸå¾…ä¸­ã ãŒéæ•°å­—è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—: '{line}'")
                    else:
                        # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåã‚’æœŸå¾…ã—ã¦ã„ã‚‹å ´åˆ
                        if not line.isdigit():
                            # æ•°å­—ä»¥å¤–ã®è¡Œã‚’ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåã¨ã—ã¦æŠ½å‡º
                            client_names.append(line)
                            extracted_count += 1
                            st.write(f"âœ… æŠ½å‡ºï¼ˆåå‰ï¼‰: '{line}'")
                            expect_id = True  # æ¬¡ã¯IDã‚’æœŸå¾…
                        else:
                            # æ•°å­—ã®è¡ŒãŒæ¥ãŸå ´åˆã€ã“ã‚Œã¯æ¬¡ã®IDã®å¯èƒ½æ€§
                            st.write(f"ğŸ”¢ æ¬¡ã®IDç™ºè¦‹: '{line}'")
                            expect_id = False  # æ¬¡ã¯ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåã‚’æœŸå¾…
            
                st.write(f"ğŸ“Š ã“ã®ãƒšãƒ¼ã‚¸ã‹ã‚‰ {extracted_count} ä»¶ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåã‚’æŠ½å‡ºã—ã¾ã—ãŸ")

    except Exception as e:
        st.error(f"ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåæŠ½å‡ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        st.exception(e)

    # çµæœã®è¡¨ç¤º
    st.write(f"ğŸ¯ æœ€çµ‚çµæœ: ç· {len(client_names)} ä»¶ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåã‚’æŠ½å‡º")
    if client_names:
        st.write("æŠ½å‡ºã•ã‚ŒãŸã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå:")
        for i, name in enumerate(client_names[:10]):  # æœ€åˆã®10ä»¶ã‚’è¡¨ç¤º
            st.write(f"  {i+1}: {name}")
        if len(client_names) > 10:
            st.write(f"  ... ä»– {len(client_names) - 10} ä»¶")

    return client_names

# PDF â†’ Excel å¤‰æ› ãƒšãƒ¼ã‚¸
if page_selection == "PDF â†’ Excel å¤‰æ›":
    st.markdown('<div class="title">ã€æ•°å‡ºè¡¨ã€‘PDF â†’ Excelã¸ã®å¤‰æ›</div>', unsafe_allow_html=True)
    st.markdown('<div class="subtitle">PDFã®æ•°å‡ºè¡¨ã‚’Excelã«å¤‰æ›ã—ã€åŒæ™‚ã«ç››ã‚Šä»˜ã‘æœ­ã‚’ä½œæˆã—ã¾ã™ã€‚</div>', unsafe_allow_html=True)

    # UIï¼šPDFãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    uploaded_pdf = st.file_uploader("å‡¦ç†ã™ã‚‹PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„", type="pdf",
                                    help="ã“ã“ã«PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‰ãƒ©ãƒƒã‚°ï¼†ãƒ‰ãƒ­ãƒƒãƒ—ã™ã‚‹ã‹ã€ã‚¯ãƒªãƒƒã‚¯ã—ã¦é¸æŠã—ã¦ãã ã•ã„ã€‚")

    # ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã¨ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰è¡¨ç¤ºç”¨ã®ã‚³ãƒ³ãƒ†ãƒŠ
    file_container = st.container()
    download_container = st.container()

    # PDFãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸã‚‰å‡¦ç†ã‚’å®Ÿè¡Œ
    if uploaded_pdf is not None and st.session_state.template_wb is not None:
        # å‡¦ç†ä¸­ã®è¡¨ç¤º
        with file_container:
            file_ext = uploaded_pdf.name.split('.')[-1].lower()
            file_icon = "PDF"
            file_size = len(uploaded_pdf.getvalue()) / 1024

            progress_placeholder = st.empty()
            progress_placeholder.markdown(f"""
            <div class="file-card">
                <div class="file-info">
                    <div class="file-icon">{file_icon}</div>
                    <div class="file-details">
                        <div class="file-name">{uploaded_pdf.name}</div>
                        <div class="file-meta">{file_size:.0f} KB</div>
                    </div>
                </div>
                <div class="loading-spinner"></div>
            </div>
            <div class="progress-bar"><div class="progress-value"></div></div>
            """, unsafe_allow_html=True)

        # PDFã®ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿ã‚’io.BytesIOã«å¤‰æ›
        pdf_bytes_io = io.BytesIO(uploaded_pdf.getvalue())

        # DataFrameã¸ã®å¤‰æ›ï¼ˆè²¼ã‚Šä»˜ã‘ç”¨ã‚·ãƒ¼ãƒˆå‘ã‘ï¼‰
        df_paste_sheet = None
        with st.spinner("ã€Œè²¼ã‚Šä»˜ã‘ç”¨ã€ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºä¸­..."):
            pdf_bytes_io.seek(0) 
            df_paste_sheet = pdf_to_excel_data_for_paste_sheet(pdf_bytes_io)

        # DataFrameã¸ã®å¤‰æ›ï¼ˆæ³¨æ–‡å¼å½“ã®æŠ½å‡ºã‚·ãƒ¼ãƒˆå‘ã‘ï¼‰
        df_bento_sheet = None
        if df_paste_sheet is not None:
            with st.spinner("ã€Œæ³¨æ–‡å¼å½“ã®æŠ½å‡ºã€ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºä¸­..."):
                try:
                    pdf_bytes_io.seek(0)
                    tables = extract_table_from_pdf_for_bento(pdf_bytes_io)
                    if not tables:
                        st.warning("PDFã‹ã‚‰è¡¨ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ï¼ˆæ³¨æ–‡å¼å½“ã®æŠ½å‡ºï¼‰")
                    else:
                        main_table = max(tables, key=lambda t: len(t) * len(t[0])) if tables else []
                        if not main_table:
                            st.warning("ãƒ¡ã‚¤ãƒ³ã¨ãªã‚‹è¡¨ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ï¼ˆæ³¨æ–‡å¼å½“ã®æŠ½å‡ºï¼‰")
                        else:
                            anchor_col = find_correct_anchor_for_bento(main_table)
                            if anchor_col == -1:
                                st.warning("ã€Œèµ¤ã€è¡Œä¸‹ã®ã€Œé£¯ãªã—ã€ã‚’è¦‹ã¤ã‘ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚ï¼ˆæ³¨æ–‡å¼å½“ã®æŠ½å‡ºï¼‰")
                            else:
                                bento_list = extract_bento_range_for_bento(main_table, anchor_col)
                                if not bento_list:
                                    st.warning("å¼å½“ç¯„å›²ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ï¼ˆæ³¨æ–‡å¼å½“ã®æŠ½å‡ºï¼‰")
                                else:
                                    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã®ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
                                    matched_list = match_bento_names(bento_list, st.session_state.master_df)
                                    output_data_bento = []
                                    for item in matched_list:
                                        match_found = False
                                        match = re.search(r' \(å…¥æ•°: (.+?)\)$', item)
                                        if match:
                                            bento_name = item[:match.start()]
                                            bento_count = match.group(1)
                                            output_data_bento.append([bento_name.strip(), bento_count.strip()])
                                            match_found = True
                                        elif "(æœªãƒãƒƒãƒ)" in item:
                                            bento_name = item.replace(" (æœªãƒãƒƒãƒ)", "").strip()
                                            bento_count = ""
                                            output_data_bento.append([bento_name, bento_count])
                                            match_found = True
                                        if not match_found:
                                            output_data_bento.append([item.strip(), ""])
                                    df_bento_sheet = pd.DataFrame(output_data_bento, columns=['å•†å“äºˆå®šå', 'ãƒ‘ãƒ³ç®±å…¥æ•°'])
                except Exception as e:
                    st.error(f"ã€Œæ³¨æ–‡å¼å½“ã®æŠ½å‡ºã€ãƒ‡ãƒ¼ã‚¿å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                    st.exception(e)

        # DataFrameã¸ã®å¤‰æ›ï¼ˆã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæŠ½å‡ºã‚·ãƒ¼ãƒˆå‘ã‘ï¼‰
        df_client_sheet = None
        if df_paste_sheet is not None:
            with st.spinner("ã€Œã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæŠ½å‡ºã€ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºä¸­..."):
                try:
                    pdf_bytes_io.seek(0)
                    client_names = extract_client_names_from_pdf(pdf_bytes_io)
                    if not client_names:
                        st.warning("PDFã‹ã‚‰ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                    else:
                        # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåã‚’DataFrameã«å¤‰æ›
                        output_data_client = [[name] for name in client_names]
                        df_client_sheet = pd.DataFrame(output_data_client, columns=['ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå'])
                except Exception as e:
                    st.error(f"ã€Œã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæŠ½å‡ºã€ãƒ‡ãƒ¼ã‚¿å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                    st.exception(e)

        # Excelã«æ›¸ãè¾¼ã¿
        if df_paste_sheet is not None:
            try:
                with st.spinner("Excelãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«ãƒ‡ãƒ¼ã‚¿ã‚’æ›¸ãè¾¼ã¿ä¸­..."):
                    try:
                        ws_paste = st.session_state.template_wb["è²¼ã‚Šä»˜ã‘ç”¨"]
                        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªã‚¢ã—ã¦ã‹ã‚‰æ›¸ãè¾¼ã‚€å ´åˆã¯ä»¥ä¸‹ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’è§£é™¤
                        # ws_paste.delete_rows(1, ws_paste.max_row)
                        for r_idx, row in df_paste_sheet.iterrows():
                            for c_idx, value in enumerate(row):
                                ws_paste.cell(row=r_idx + 1, column=c_idx + 1, value=value)
                    except KeyError:
                        st.error("ã‚¨ãƒ©ãƒ¼: ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«ã€Œè²¼ã‚Šä»˜ã‘ç”¨ã€ã¨ã„ã†åå‰ã®ã‚·ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                        st.stop()
                    
                    if df_bento_sheet is not None and not df_bento_sheet.empty:
                        try:
                            ws_bento = st.session_state.template_wb["æ³¨æ–‡å¼å½“ã®æŠ½å‡º"]
                            # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªã‚¢ã—ã¦ã‹ã‚‰æ›¸ãè¾¼ã‚€å ´åˆã¯ä»¥ä¸‹ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’è§£é™¤
                            # ws_bento.delete_rows(1, ws_bento.max_row)
                            for r_idx, row in df_bento_sheet.iterrows():
                                for c_idx, value in enumerate(row):
                                    ws_bento.cell(row=r_idx + 1, column=c_idx + 1, value=value)
                        except KeyError:
                            st.error("ã‚¨ãƒ©ãƒ¼: ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«ã€Œæ³¨æ–‡å¼å½“ã®æŠ½å‡ºã€ã¨ã„ã†åå‰ã®ã‚·ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                            st.stop()
                    elif df_bento_sheet is not None and df_bento_sheet.empty:
                        st.warning("ã€Œæ³¨æ–‡å¼å½“ã®æŠ½å‡ºã€ã‚·ãƒ¼ãƒˆã«æ›¸ãè¾¼ã‚€ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                    else:
                        st.warning("ã€Œæ³¨æ–‡å¼å½“ã®æŠ½å‡ºã€ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ãŒã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã“ã®ã‚·ãƒ¼ãƒˆã¸ã®æ›¸ãè¾¼ã¿ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ã€‚")

                    # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæŠ½å‡ºã‚·ãƒ¼ãƒˆã¸ã®æ›¸ãè¾¼ã¿
                    if df_client_sheet is not None and not df_client_sheet.empty:
                        try:
                            ws_client = st.session_state.template_wb["ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæŠ½å‡º"]
                            # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªã‚¢ã—ã¦ã‹ã‚‰æ›¸ãè¾¼ã‚€å ´åˆã¯ä»¥ä¸‹ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’è§£é™¤
                            # ws_client.delete_rows(1, ws_client.max_row)
                            for r_idx, row in df_client_sheet.iterrows():
                                for c_idx, value in enumerate(row):
                                    ws_client.cell(row=r_idx + 1, column=c_idx + 1, value=value)
                        except KeyError:
                            st.error("ã‚¨ãƒ©ãƒ¼: ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«ã€Œã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæŠ½å‡ºã€ã¨ã„ã†åå‰ã®ã‚·ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                            st.stop()
                    elif df_client_sheet is not None and df_client_sheet.empty:
                        st.warning("ã€Œã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæŠ½å‡ºã€ã‚·ãƒ¼ãƒˆã«æ›¸ãè¾¼ã‚€ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                    else:
                        st.warning("ã€Œã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæŠ½å‡ºã€ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ãŒã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã“ã®ã‚·ãƒ¼ãƒˆã¸ã®æ›¸ãè¾¼ã¿ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ã€‚")

                # ãƒ¡ãƒ¢ãƒªä¸Šã§Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ
                with st.spinner("Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆä¸­..."):
                    output = io.BytesIO()
                    st.session_state.template_wb.save(output)
                    output.seek(0)
                    final_excel_bytes = output.read()

                # å‡¦ç†å®Œäº†è¡¨ç¤º
                with file_container:
                        progress_placeholder.markdown(f"""
                        <div class="file-card">
                            <div class="file-info">
                                <div class="file-icon">{file_icon}</div>
                                <div class="file-details">
                                    <div class="file-name">{uploaded_pdf.name}</div>
                                    <div class="file-meta">{file_size:.0f} KB - å‡¦ç†å®Œäº†</div>
                                </div>
                            </div>
                            <div class="check-icon">âœ“</div>
                        </div>
                        """, unsafe_allow_html=True)

                # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒªãƒ³ã‚¯ã®ç”Ÿæˆ
                with download_container:
                    st.markdown('<div class="separator"></div>', unsafe_allow_html=True)

                    original_pdf_name = os.path.splitext(uploaded_pdf.name)[0]
                    output_filename = f"{original_pdf_name}_Processed.xlsm"
                    excel_size = len(final_excel_bytes) / 1024
                    b64 = base64.b64encode(final_excel_bytes).decode('utf-8')

                    mime_type = "application/vnd.ms-excel.sheet.macroEnabled.12"

                    href = f"""
                    <a href="data:{mime_type};base64,{b64}" download="{output_filename}" class="download-card">
                        <div class="download-info">
                            <div class="download-icon">XLSM</div>
                            <div class="download-details">
                                <div class="download-name">{output_filename}</div>
                                <div class="download-meta">Excel (ãƒã‚¯ãƒ­æœ‰åŠ¹)ãƒ»{excel_size:.0f} KB</div>
                            </div>
                        </div>
                        <div class="download-button-imitation">
                            <span class="download-button-icon">â†“</span>
                            Download
                        </div>
                    </a>
                    """
                    st.markdown(href, unsafe_allow_html=True)

            except Exception as e:
                st.error(f"Excelãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®æ›¸ãè¾¼ã¿ã¾ãŸã¯ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                st.exception(e)
                with file_container:
                        progress_placeholder.markdown(f"ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}", unsafe_allow_html=True)

# ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæƒ…å ±æŠ½å‡º ãƒšãƒ¼ã‚¸
elif page_selection == "ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæƒ…å ±æŠ½å‡º":
    st.markdown('<div class="title">ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæƒ…å ±æŠ½å‡º</div>', unsafe_allow_html=True)
    st.markdown('<div class="subtitle">PDFã‹ã‚‰ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåã¨çµ¦é£Ÿã®æ•°ã‚’æŠ½å‡ºã—ã¦CSVãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›ã—ã¾ã™ã€‚</div>', unsafe_allow_html=True)

    # PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    uploaded_file = st.file_uploader("PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„", type="pdf")
    
    if uploaded_file is not None:
        # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæƒ…å ±ã‚’æŠ½å‡º
        client_data = extract_client_and_meal_info_from_pdf(uploaded_file)
        
        if client_data:
            # ãƒ‡ãƒ¼ã‚¿ã‚’DataFrameã«å¤‰æ›
            df = export_client_data_to_csv(client_data)
            
            # ãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º
            st.success(f"ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã¾ã—ãŸ: {len(client_data)} ä»¶")
            st.dataframe(df)
            
            # CSVãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
            csv_data = df.to_csv(index=False, encoding='utf-8-sig')
            
            # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
            st.download_button(
                label="ğŸ“¥ CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=csv_data,
                file_name="client_meal_data.csv",
                mime="text/csv",
                help="ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæƒ…å ±ã¨çµ¦é£Ÿã®æ•°ã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™"
            )
        else:
            st.warning("ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæƒ…å ±ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚PDFã®å½¢å¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

# ãƒã‚¹ã‚¿è¨­å®š ãƒšãƒ¼ã‚¸
elif page_selection == "ãƒã‚¹ã‚¿è¨­å®š":
    st.markdown('<div class="title">ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿è¨­å®š</div>', unsafe_allow_html=True)
    st.markdown('<div class="subtitle">å•†å“ãƒã‚¹ã‚¿ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦æ›´æ–°ã—ã¾ã™ã€‚ç¾åœ¨ã®ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚‚ç¢ºèªã§ãã¾ã™ã€‚</div>', unsafe_allow_html=True)

    # --- ãƒã‚¹ã‚¿CSVã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ ---
    master_csv_path = "å•†å“ãƒã‚¹ã‚¿ä¸€è¦§.csv"

    # --- UI: æ–°ã—ã„ãƒã‚¹ã‚¿CSVã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ ---
    st.markdown("#### æ–°ã—ã„ãƒã‚¹ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    uploaded_master_csv = st.file_uploader(
        "å•†å“ãƒã‚¹ã‚¿ä¸€è¦§.csv ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„",
        type="csv",
        help="ãƒ˜ãƒƒãƒ€ãƒ¼ã«ã¯ 'å•†å“äºˆå®šå' ã¨ 'ãƒ‘ãƒ³ç®±å…¥æ•°' ã‚’å«ã‚ã¦ãã ã•ã„ã€‚"
    )

    if uploaded_master_csv is not None:
        try:
            # --- ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸCSVã‚’DataFrameã¨ã—ã¦èª­ã¿è¾¼ã‚€ ---
            new_master_df = None
            # BOMä»˜ãUTF-8ã€Shift_JISãªã©ã€è¤‡æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è©¦ã™
            encodings = ['utf-8-sig', 'utf-8', 'cp932', 'shift_jis']
            for encoding in encodings:
                try:
                    uploaded_master_csv.seek(0) # ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚¤ãƒ³ã‚¿ã‚’å…ˆé ­ã«æˆ»ã™
                    temp_df = pd.read_csv(uploaded_master_csv, encoding=encoding)
                    # å¿…é ˆã‚«ãƒ©ãƒ ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
                    if 'å•†å“äºˆå®šå' in temp_df.columns and 'ãƒ‘ãƒ³ç®±å…¥æ•°' in temp_df.columns:
                        new_master_df = temp_df
                        st.info(f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ {encoding} ã§èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
                        break
                    else:
                        st.warning(f"{encoding} ã§èª­ã¿è¾¼ã¿ã¾ã—ãŸãŒã€å¿…é ˆåˆ—ï¼ˆ'å•†å“äºˆå®šå', 'ãƒ‘ãƒ³ç®±å…¥æ•°'ï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

                except (UnicodeDecodeError, pd.errors.ParserError):
                    continue # æ¬¡ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è©¦ã™
                except Exception as e:
                    st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                    break

            if new_master_df is not None:
                # --- ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã‚’æ›´æ–° ---
                st.session_state.master_df = new_master_df

                # --- âœ… CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¸Šæ›¸ãä¿å­˜ ---
                try:
                    # UTF-8 (BOMä»˜ã)ã§ä¿å­˜ã€‚Excelã§ã®æ–‡å­—åŒ–ã‘ã‚’é˜²ã
                    new_master_df.to_csv(master_csv_path, index=False, encoding='utf-8-sig')
                    st.success(f"âœ… ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°ã—ã€'{master_csv_path}' ã«ä¸Šæ›¸ãä¿å­˜ã—ã¾ã—ãŸã€‚")
                    st.info("ã‚¢ãƒ—ãƒªã‚’å†èµ·å‹•ã—ã¦ã‚‚ã€ã“ã®ãƒã‚¹ã‚¿ãŒèª­ã¿è¾¼ã¾ã‚Œã¾ã™ã€‚")

                except Exception as e:
                    st.error(f"ãƒã‚¹ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                    st.exception(e)

            else:
                st.error("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸCSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ­£ã—ãèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ•ã‚¡ã‚¤ãƒ«ã®å½¢å¼ï¼ˆå¿…é ˆåˆ—ã®æœ‰ç„¡ï¼‰ã‚„ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

        except Exception as e:
            st.error(f"ãƒã‚¹ã‚¿æ›´æ–°å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            st.exception(e)

    st.markdown('<div class="separator"></div>', unsafe_allow_html=True)

    # --- ç¾åœ¨ã®ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º ---
    st.markdown("#### ç¾åœ¨ã®ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿")
    if 'master_df' in st.session_state and not st.session_state.master_df.empty:
        st.dataframe(st.session_state.master_df, use_container_width=True)
    else:
        st.warning("ç¾åœ¨ã€ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

# ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒŠçµ‚äº†
st.markdown('</div>', unsafe_allow_html=True)
