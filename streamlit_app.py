import streamlit as st
import streamlit.components.v1 as components
import pdfplumber
import pandas as pd
import io
import re
import base64
import os
import unicodedata
import traceback
from typing import List, Dict, Any
from openpyxl import load_workbook
from openpyxl.utils.dataframe import dataframe_to_rows

# âœ… ä¿®æ­£: st.set_page_config() ã‚’æœ€åˆã«ç§»å‹•
st.set_page_config(
    page_title="ã€æ•°å‡ºè¡¨ã€‘PDF â†’ Excelã¸ã®å¤‰æ›",
    page_icon="./static/favicon.ico",
    layout="centered",
)

# --- Streamlit Session Stateã®åˆæœŸåŒ– ---
# ï¼ˆã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯å¤‰æ›´ã‚ã‚Šã¾ã›ã‚“ï¼‰
if 'master_df' not in st.session_state:
    master_csv_path = "å•†å“ãƒã‚¹ã‚¿ä¸€è¦§.csv"
    initial_master_df = None
    if os.path.exists(master_csv_path):
        encodings = ['utf-8-sig', 'utf-8', 'cp932', 'shift_jis', 'euc-jp', 'iso-2022-jp']
        for encoding in encodings:
            try:
                temp_df = pd.read_csv(master_csv_path, encoding=encoding)
                if not temp_df.empty:
                    initial_master_df = temp_df
                    st.success(f"æ—¢å­˜ã®ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ {encoding} ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
                    break
            except (UnicodeDecodeError, pd.errors.EmptyDataError):
                continue
            except Exception as e:
                st.warning(f"æ—¢å­˜ãƒã‚¹ã‚¿CSV ({master_csv_path}) ã‚’ {encoding} ã§èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                continue
    if initial_master_df is None:
        st.warning(f"ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ '{master_csv_path}' ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹ã€èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚ãƒã‚¹ã‚¿è¨­å®šãƒšãƒ¼ã‚¸ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        initial_master_df = pd.DataFrame(columns=['å•†å“äºˆå®šå', 'ãƒ‘ãƒ³ç®±å…¥æ•°', 'å•†å“å']) # 'å•†å“å' ã‚’è¿½åŠ 
    st.session_state.master_df = initial_master_df


# --- ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆExcelãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ ---
# template.xlsm ã®èª­ã¿è¾¼ã¿ï¼ˆå¤‰æ›´ãªã—ï¼‰
if 'template_wb_loaded' not in st.session_state:
    st.session_state.template_wb_loaded = False
    st.session_state.template_wb = None

template_path = "template.xlsm"
if not st.session_state.template_wb_loaded:
    if not os.path.exists(template_path):
        st.error(f"ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ« '{template_path}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        st.stop()
    try:
        st.session_state.template_wb = load_workbook(template_path, keep_vba=True)
        st.session_state.template_wb_loaded = True
        st.success(f"ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ« '{template_path}' ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
    except Exception as e:
        st.error(f"ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ« '{template_path}' ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        st.stop()

# âœ…ã€å¤‰æ›´ç‚¹ã€‘nouhinsyo.xlsx ã®èª­ã¿è¾¼ã¿ã‚’è¿½åŠ 
if 'nouhinsyo_wb_loaded' not in st.session_state:
    st.session_state.nouhinsyo_wb_loaded = False
    st.session_state.nouhinsyo_wb = None

nouhinsyo_path = "nouhinsyo.xlsx"
if not st.session_state.nouhinsyo_wb_loaded:
    if not os.path.exists(nouhinsyo_path):
        st.error(f"ç´å“æ›¸ãƒ•ã‚¡ã‚¤ãƒ« '{nouhinsyo_path}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        st.stop()
    try:
        st.session_state.nouhinsyo_wb = load_workbook(nouhinsyo_path)
        st.session_state.nouhinsyo_wb_loaded = True
        st.success(f"ç´å“æ›¸ãƒ•ã‚¡ã‚¤ãƒ« '{nouhinsyo_path}' ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
    except Exception as e:
        st.error(f"ç´å“æ›¸ãƒ•ã‚¡ã‚¤ãƒ« '{nouhinsyo_path}' ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        st.stop()


# --- HTML/CSS, ã‚µã‚¤ãƒ‰ãƒãƒ¼, é–¢æ•°å®šç¾© ---
# (ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯å¤‰æ›´ã‚ã‚Šã¾ã›ã‚“ã€‚é•·ã„ã®ã§çœç•¥ã—ã¾ã™)
# PWAç”¨HTMLåŸ‹ã‚è¾¼ã¿
components.html(
    """
    <link rel="manifest" href="./static/manifest.json">
    <link rel="icon" href="./static/favicon.ico">
    <link rel="apple-touch-icon" sizes="180x180" href="./static/icons/apple-touch-icon.png">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-title" content="PDFConverter">
    """,
    height=0,
)
# CSSã‚¹ã‚¿ã‚¤ãƒ«
st.markdown("""
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600&family=Roboto:wght@300;400;500&display=swap');
        .stApp { background: #fff5e6; font-family: 'Inter', sans-serif; }
        .title { font-size: 1.5rem; font-weight: 600; color: #333; margin-bottom: 5px; }
        .subtitle { font-size: 0.9rem; color: #666; margin-bottom: 25px; }
    </style>
""", unsafe_allow_html=True)
# ã‚µã‚¤ãƒ‰ãƒãƒ¼ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³
st.sidebar.title("ãƒ¡ãƒ‹ãƒ¥ãƒ¼")
page_selection = st.sidebar.radio("è¡¨ç¤ºã™ã‚‹æ©Ÿèƒ½ã‚’é¸æŠã—ã¦ãã ã•ã„", ("PDF â†’ Excel å¤‰æ›", "ãƒã‚¹ã‚¿è¨­å®š"), index=0)
st.markdown("---")
# é–¢æ•°å®šç¾©... (å¤‰æ›´ãªã—)
def extract_detailed_client_info_from_pdf(pdf_file_obj):
    """PDFã‹ã‚‰è©³ç´°ãªã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæƒ…å ±ï¼ˆåå‰ï¼‹çµ¦é£Ÿã®æ•°ï¼‰ã‚’æŠ½å‡ºã™ã‚‹"""
    client_data = []

    try:
        with pdfplumber.open(pdf_file_obj) as pdf:
            for page_num, page in enumerate(pdf.pages):
                # è¡¨å½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                rows = extract_text_with_layout(page)
                if not rows:
                    continue

                # åœ’åã®ä½ç½®ã‚’æ¢ã™
                garden_row_idx = -1
                for i, row in enumerate(rows):
                    row_text = ''.join(str(cell) for cell in row if cell)
                    if 'åœ’å' in row_text:
                        garden_row_idx = i
                        break

                if garden_row_idx == -1:
                    continue

                # åœ’åã‚ˆã‚Šä¸‹ã®è¡Œã‚’å‡¦ç†
                current_client_id = None
                current_client_name = None

                for i in range(garden_row_idx + 1, len(rows)):
                    row = rows[i]

                    # 10001ãŒå‡ºã¦ããŸã‚‰çµ‚äº†
                    row_text = ''.join(str(cell) for cell in row if cell)
                    if '10001' in row_text:
                        break

                    # ç©ºè¡Œã¯ã‚¹ã‚­ãƒƒãƒ—
                    if not any(str(cell).strip() for cell in row):
                        continue

                    # å·¦ã®åˆ—ï¼ˆ1ç•ªç›®ã®åˆ—ï¼‰ã‚’ãƒã‚§ãƒƒã‚¯
                    if len(row) > 0 and row[0]:
                        left_cell = str(row[0]).strip()

                        # æ•°å­—ã ã‘ã®å ´åˆã¯ID
                        if re.match(r'^\d+$', left_cell):
                            # å‰ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
                            if current_client_id and current_client_name:
                                client_info = extract_meal_numbers_from_row(rows, i-1, current_client_id, current_client_name)
                                if client_info:
                                    client_data.append(client_info)

                            current_client_id = left_cell
                            current_client_name = None

                        # æ•°å­—ä»¥å¤–ã®å ´åˆã¯ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå
                        elif not re.match(r'^\d+$', left_cell) and current_client_id:
                            current_client_name = left_cell

                # æœ€å¾Œã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
                if current_client_id and current_client_name:
                    client_info = extract_meal_numbers_from_row(rows, len(rows)-1, current_client_id, current_client_name)
                    if client_info:
                        client_data.append(client_info)

    except Exception as e:
        st.error(f"ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæƒ…å ±æŠ½å‡ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

    return client_data

def extract_meal_numbers_from_row(rows, row_idx, client_id, client_name):
    """æŒ‡å®šã•ã‚ŒãŸè¡Œã¨ãã®å‘¨è¾ºã‹ã‚‰çµ¦é£Ÿã®æ•°ã‚’æŠ½å‡º"""
    client_info = {
        'client_id': client_id,
        'client_name': client_name,
        'student_meals': [],
        'teacher_meals': []
    }

    # IDã®è¡Œã¨ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåã®è¡Œã‹ã‚‰æ•°å­—ã‚’æŠ½å‡º
    rows_to_check = []

    # IDã®è¡Œã‚’æ¢ã™
    id_row_idx = -1
    name_row_idx = -1

    for i in range(max(0, row_idx - 3), min(len(rows), row_idx + 3)):
        if i < len(rows) and len(rows[i]) > 0:
            left_cell = str(rows[i][0]).strip()
            if left_cell == client_id:
                id_row_idx = i
                rows_to_check.append(('id', i, rows[i]))
            elif left_cell == client_name:
                name_row_idx = i
                rows_to_check.append(('name', i, rows[i]))

    # æ•°å­—ã‚’æŠ½å‡º
    all_numbers = []

    for row_type, idx, row in rows_to_check:
        # å·¦ã®åˆ—ï¼ˆ0ç•ªç›®ï¼‰ä»¥å¤–ã®åˆ—ã‹ã‚‰æ•°å­—ã‚’æŠ½å‡º
        for col_idx in range(1, len(row)):
            cell = str(row[col_idx]).strip()
            if cell and re.match(r'^\d+$', cell):
                all_numbers.append({
                    'number': int(cell),
                    'row_type': row_type,
                    'col_idx': col_idx
                })
            elif cell and not re.match(r'^\d+$', cell) and cell != '':
                # æ•°å­—ä»¥å¤–ã®æ–‡å­—ãŒå‡ºã¦ããŸã‚‰ãã®è¡Œã¯ã“ã“ã§çµ‚äº†
                break

    # åœ’å…ã®çµ¦é£Ÿã®æ•°ã¨å…ˆç”Ÿã®çµ¦é£Ÿã®æ•°ã«åˆ†ã‘ã‚‹
    # IDã®è¡Œã®æ•°å­—ã¯åœ’å…ã®çµ¦é£Ÿã®æ•°
    # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåã®è¡Œã®æ•°å­—ã¯å…ˆç”Ÿã®çµ¦é£Ÿã®æ•°

    id_numbers = [item['number'] for item in all_numbers if item['row_type'] == 'id']
    name_numbers = [item['number'] for item in all_numbers if item['row_type'] == 'name']

    # åœ’å…ã®çµ¦é£Ÿã®æ•°ï¼ˆæœ€å¤§3ã¤ï¼‰
    client_info['student_meals'] = id_numbers[:3]

    # å…ˆç”Ÿã®çµ¦é£Ÿã®æ•°ï¼ˆæœ€å¤§2ã¤ï¼‰
    client_info['teacher_meals'] = name_numbers[:2]

    return client_info

def export_detailed_client_data_to_dataframe(client_data):
    """è©³ç´°ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæƒ…å ±ã‚’DataFrameã«å¤‰æ›"""
    df_data = []

    for client_info in client_data:
        row = {
            'ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå': client_info['client_name'],
            'åœ’å…ã®çµ¦é£Ÿã®æ•°1': client_info['student_meals'][0] if len(client_info['student_meals']) > 0 else '',
            'åœ’å…ã®çµ¦é£Ÿã®æ•°2': client_info['student_meals'][1] if len(client_info['student_meals']) > 1 else '',
            'åœ’å…ã®çµ¦é£Ÿã®æ•°3': client_info['student_meals'][2] if len(client_info['student_meals']) > 2 else '',
            'å…ˆç”Ÿã®çµ¦é£Ÿã®æ•°1': client_info['teacher_meals'][0] if len(client_info['teacher_meals']) > 0 else '',
            'å…ˆç”Ÿã®çµ¦é£Ÿã®æ•°2': client_info['teacher_meals'][1] if len(client_info['teacher_meals']) > 1 else '',
        }
        df_data.append(row)

    return pd.DataFrame(df_data)

def is_number(text: str) -> bool:
    return bool(re.match(r'^\d+$', text.strip()))

def get_line_groups(words: List[Dict[str, Any]], y_tolerance: float = 1.2) -> List[List[Dict[str, Any]]]:
    if not words:
        return []
    sorted_words = sorted(words, key=lambda w: w['top'])
    groups = []
    current_group = [sorted_words[0]]
    current_top = sorted_words[0]['top']
    for word in sorted_words[1:]:
        if abs(word['top'] - current_top) <= y_tolerance:
            current_group.append(word)
        else:
            groups.append(current_group)
            current_group = [word]
            current_top = word['top']
    groups.append(current_group)
    return groups

def get_vertical_boundaries(page, tolerance: float = 2) -> List[float]:
    vertical_lines_x = []
    for line in page.lines:
        if abs(line['x0'] - line['x1']) < tolerance:
            vertical_lines_x.append((line['x0'] + line['x1']) / 2)
    vertical_lines_x = sorted(list(set(round(x, 1) for x in vertical_lines_x)))

    words = page.extract_words()
    if not words:
        return vertical_lines_x

    left_boundary = min(word['x0'] for word in words)
    right_boundary = max(word['x1'] for word in words)

    boundaries = sorted(list(set([round(left_boundary, 1)] + vertical_lines_x + [round(right_boundary, 1)])))

    merged_boundaries = []
    if boundaries:
        merged_boundaries.append(boundaries[0])
        for i in range(1, len(boundaries)):
            if boundaries[i] - merged_boundaries[-1] > tolerance * 2:
                merged_boundaries.append(boundaries[i])
        if right_boundary > merged_boundaries[-1] + tolerance * 2:
            merged_boundaries.append(round(right_boundary, 1))
        boundaries = sorted(list(set(merged_boundaries)))

    return boundaries

def split_line_using_boundaries(sorted_words_in_line: List[Dict[str, Any]], boundaries: List[float]) -> List[str]:
    columns = [""] * (len(boundaries) - 1)
    for word in sorted_words_in_line:
        word_center_x = (word['x0'] + word['x1']) / 2
        for i in range(len(boundaries) - 1):
            left = boundaries[i]
            right = boundaries[i + 1]
            if left <= word_center_x < right:
                if columns[i]:
                    columns[i] += " " + word["text"]
                else:
                    columns[i] = word["text"]
                break
    return columns

def extract_text_with_layout(page) -> List[List[str]]:
    words = page.extract_words(x_tolerance=3, y_tolerance=3, keep_blank_chars=False)
    if not words:
        return []

    boundaries = get_vertical_boundaries(page)
    if len(boundaries) < 2:
        lines = page.extract_text(layout=False, x_tolerance=3, y_tolerance=3)
        return [[line] for line in lines.split('\n') if line.strip()]

    row_groups = get_line_groups(words, y_tolerance=1.5)

    result_rows = []
    for group in row_groups:
        sorted_group = sorted(group, key=lambda w: w['x0'])
        columns = split_line_using_boundaries(sorted_group, boundaries)
        if any(cell.strip() for cell in columns):
            result_rows.append(columns)

    return result_rows

def remove_extra_empty_columns(rows: List[List[str]]) -> List[List[str]]:
    if not rows:
        return rows
    num_cols = max(len(row) for row in rows) if rows else 0
    if num_cols == 0:
        return rows

    is_col_empty = [True] * num_cols
    for r, row in enumerate(rows):
        for c in range(len(row)):
            if c < num_cols and row[c].strip():
                is_col_empty[c] = False

    keep_indices = [c for c in range(num_cols) if not is_col_empty[c]]

    new_rows = []
    for row in rows:
        new_row = [row[i] if i < len(row) else "" for i in keep_indices]
        new_rows.append(new_row)

    return new_rows

def post_process_rows(rows: List[List[str]]) -> List[List[str]]:
    new_rows = [row[:] for row in rows]
    for i, row in enumerate(new_rows):
        for j, cell in enumerate(row):
            if "åˆè¨ˆ" in str(cell):
                if i > 0 and j < len(new_rows[i-1]):
                    new_rows[i-1][j] = ""
    return new_rows

def pdf_to_excel_data_for_paste_sheet(pdf_file) -> pd.DataFrame | None:
    try:
        with pdfplumber.open(pdf_file) as pdf:
            if not pdf.pages:
                st.warning("PDFã«ãƒšãƒ¼ã‚¸ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                return None
            page = pdf.pages[0]

            rows = extract_text_with_layout(page)
            rows = [row for row in rows if any(cell.strip() for cell in row)]
            if not rows:
                st.warning("PDFã®æœ€åˆã®ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                return None

            rows = post_process_rows(rows)
            rows = remove_extra_empty_columns(rows)
            if not rows or not rows[0]:
                st.warning("ç©ºã®åˆ—ã‚’å‰Šé™¤ã—ãŸçµæœã€ãƒ‡ãƒ¼ã‚¿ãŒãªããªã‚Šã¾ã—ãŸã€‚")
                return None

            max_cols = max(len(row) for row in rows) if rows else 0
            normalized_rows = [row + [''] * (max_cols - len(row)) for row in rows]
            df = pd.DataFrame(normalized_rows)
            return df

    except Exception as e:
        st.error(f"PDFå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        traceback.print_exc()
        return None

def extract_table_from_pdf_for_bento(pdf_file_obj):
    tables = []
    with pdfplumber.open(pdf_file_obj) as pdf:
        for page in pdf.pages:
            text = page.extract_text()
            if text is None: continue

            start_keywords = ["åœ’å", "é£¯ã‚ã‚Š", "ã‚­ãƒ£ãƒ©å¼"]
            end_keywords = ["ãŠã‚„ã¤", "åˆè¨ˆ", "PAGE"]

            if not any(kw in text for kw in start_keywords):
                continue

            lines = page.lines
            if not lines:
                continue

            y_coords = sorted(set([line['top'] for line in lines] + [line['bottom'] for line in lines]))
            if len(y_coords) < 2:
                continue

            table_top = min(y_coords)
            table_bottom = max(y_coords)

            x_coords = sorted(set([line['x0'] for line in lines] + [line['x1'] for line in lines]))
            if len(x_coords) < 2:
                continue

            table_left = min(x_coords)
            table_right = max(x_coords)

            table_bbox = (table_left, table_top, table_right, table_bottom)
            cropped_page = page.crop(table_bbox)

            table_settings = {
                "vertical_strategy": "lines",
                "horizontal_strategy": "lines",
                "snap_tolerance": 3,
                "join_tolerance": 3,
                "edge_min_length": 15,
            }

            table = cropped_page.extract_table(table_settings)
            if table:
                tables.append(table)

    return tables

def find_correct_anchor_for_bento(table, target_row_text="èµ¤"):
    for row_idx, row in enumerate(table):
        row_text = ''.join(str(cell) for cell in row if cell)
        if target_row_text in row_text:
            for offset in [1, 2]:
                if row_idx + offset < len(table):
                    next_row = table[row_idx + offset]
                    for col_idx, cell in enumerate(next_row):
                        if cell and "é£¯ãªã—" in cell:
                            return col_idx
    return -1

def extract_bento_range_for_bento(table, start_col):
    bento_list = []
    end_col = -1

    for row in table:
        row_text = ''.join(str(cell) for cell in row if cell)
        if "ãŠã‚„ã¤" in row_text:
            for col_idx, cell in enumerate(row):
                if cell and "ãŠã‚„ã¤" in cell:
                    end_col = col_idx
                    break
            if end_col != -1:
                break

    if end_col == -1 or start_col >= end_col:
        return []

    header_row_idx = None
    anchor_row_idx = -1
    for row_idx, row in enumerate(table):
        if any(cell and "é£¯ãªã—" in cell for cell in row):
            anchor_row_idx = row_idx
            break

    if anchor_row_idx == -1:
        return []

    if anchor_row_idx - 1 >= 0:
        header_row_idx = anchor_row_idx - 1
    else:
        return []

    for col in range(start_col + 1, end_col + 1):
        if col < len(table[header_row_idx]):
            cell_text = table[header_row_idx][col]
        else:
            cell_text = ""

        if cell_text and str(cell_text).strip() and "é£¯ãªã—" not in str(cell_text):
            bento_list.append(str(cell_text).strip())

    return bento_list

def match_bento_names(pdf_bento_list, master_df):
    if master_df is None or master_df.empty:
        st.error("ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return [f"{name} (ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ãªã—)" for name in pdf_bento_list]

    master_data_tuples = []
    try:
        if 'å•†å“äºˆå®šå' in master_df.columns and 'ãƒ‘ãƒ³ç®±å…¥æ•°' in master_df.columns:
            master_data_tuples = master_df[['å•†å“äºˆå®šå', 'ãƒ‘ãƒ³ç®±å…¥æ•°']].dropna().values.tolist()
            master_data_tuples = [(str(name), str(value)) for name, value in master_data_tuples]
        elif 'å•†å“äºˆå®šå' in master_df.columns:
            st.warning("ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ã«ã€Œãƒ‘ãƒ³ç®±å…¥æ•°ã€åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            master_data_tuples = master_df['å•†å“äºˆå®šå'].dropna().astype(str).tolist()
            master_data_tuples = [(name, "") for name in master_data_tuples]
        else:
            st.error("ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ã«ã€Œå•†å“äºˆå®šåã€åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return [f"{name} (å•†å“äºˆå®šååˆ—ãªã—)" for name in pdf_bento_list]

    except Exception as e:
        st.error(f"ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return [f"{name} (å‡¦ç†ã‚¨ãƒ©ãƒ¼)" for name in pdf_bento_list]

    if len(master_data_tuples) == 0:
        st.warning("ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æœ‰åŠ¹ãªå•†å“æƒ…å ±ãŒæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        return [f"{name} (ãƒã‚¹ã‚¿ç©º)" for name in pdf_bento_list]

    matched = []

    normalized_master_data_tuples = []
    for master_name, master_id in master_data_tuples:
        normalized_name = unicodedata.normalize('NFKC', master_name)
        normalized_name = re.sub(r'\s+', '', normalized_name)
        normalized_master_data_tuples.append((normalized_name, master_name, master_id))

    for pdf_name in pdf_bento_list:
        original_normalized_pdf_name = unicodedata.normalize('NFKC', str(pdf_name))
        original_normalized_pdf_name = re.sub(r'\s+', '', original_normalized_pdf_name)

        found_match = False
        found_original_master_name = None
        found_id = None

        for norm_m_name, orig_m_name, m_id in normalized_master_data_tuples:
            if norm_m_name.startswith(original_normalized_pdf_name):
                found_original_master_name = orig_m_name
                found_id = m_id
                found_match = True
                break

        if not found_match:
            for norm_m_name, orig_m_name, m_id in normalized_master_data_tuples:
                if original_normalized_pdf_name in norm_m_name:
                    found_original_master_name = orig_m_name
                    found_id = m_id
                    found_match = True
                    break

        if not found_match:
            for num_chars_to_remove in range(1, 4):
                if len(original_normalized_pdf_name) > num_chars_to_remove:
                    truncated_pdf_name = original_normalized_pdf_name[:-num_chars_to_remove]

                    for norm_m_name, orig_m_name, m_id in normalized_master_data_tuples:
                        if norm_m_name.startswith(truncated_pdf_name):
                            found_original_master_name = orig_m_name
                            found_id = m_id
                            found_match = True
                            break

                    if not found_match:
                        for norm_m_name, orig_m_name, m_id in normalized_master_data_tuples:
                            if truncated_pdf_name in norm_m_name:
                                found_original_master_name = orig_m_name
                                found_id = m_id
                                found_match = True
                                break

                    if found_match:
                        break

        if found_original_master_name:
            if found_id:
                matched.append(f"{found_original_master_name} (å…¥æ•°: {found_id})")
            else:
                matched.append(found_original_master_name)
        else:
            matched.append(f"{pdf_name} (æœªãƒãƒƒãƒ)")

    return matched


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# PDF â†’ Excel å¤‰æ› ãƒšãƒ¼ã‚¸
if page_selection == "PDF â†’ Excel å¤‰æ›":
    st.markdown('<div class="title">ã€æ•°å‡ºè¡¨ã€‘PDF â†’ Excelã¸ã®å¤‰æ›</div>', unsafe_allow_html=True)
    st.markdown('<div class="subtitle">PDFã®æ•°å‡ºè¡¨ã‚’Excelã«å¤‰æ›ã—ã€è©³ç´°ãªã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæƒ…å ±ã‚‚å«ã‚ã¦ä¸€æ‹¬å‡¦ç†ã—ã¾ã™ã€‚</div>', unsafe_allow_html=True)

    uploaded_pdf = st.file_uploader("å‡¦ç†ã™ã‚‹PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„", type="pdf")

    if uploaded_pdf is not None and st.session_state.template_wb is not None and st.session_state.nouhinsyo_wb is not None:
        # PDFã®ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿ã‚’io.BytesIOã«å¤‰æ›
        pdf_bytes_io = io.BytesIO(uploaded_pdf.getvalue())

        # 1. è²¼ã‚Šä»˜ã‘ç”¨ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º
        df_paste_sheet = None
        with st.spinner("ã€Œè²¼ã‚Šä»˜ã‘ç”¨ã€ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºä¸­..."):
            pdf_bytes_io.seek(0)
            df_paste_sheet = pdf_to_excel_data_for_paste_sheet(pdf_bytes_io)

        # 2. æ³¨æ–‡å¼å½“ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º
        df_bento_sheet = None
        if df_paste_sheet is not None:
            with st.spinner("ã€Œæ³¨æ–‡å¼å½“ã®æŠ½å‡ºã€ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºä¸­..."):
                try:
                    pdf_bytes_io.seek(0)
                    tables = extract_table_from_pdf_for_bento(pdf_bytes_io)
                    if tables:
                        main_table = max(tables, key=lambda t: len(t) * len(t[0]))
                        if main_table:
                            anchor_col = find_correct_anchor_for_bento(main_table)
                            if anchor_col != -1:
                                bento_list = extract_bento_range_for_bento(main_table, anchor_col)
                                if bento_list:
                                    matched_list = match_bento_names(bento_list, st.session_state.master_df)
                                    output_data_bento = []
                                    for item in matched_list:
                                        match = re.search(r' \(å…¥æ•°: (.+?)\)$', item)
                                        if match:
                                            bento_name = item[:match.start()]
                                            bento_count = match.group(1)
                                            output_data_bento.append([bento_name.strip(), bento_count.strip()])
                                        elif "(æœªãƒãƒƒãƒ)" in item:
                                            bento_name = item.replace(" (æœªãƒãƒƒãƒ)", "").strip()
                                            output_data_bento.append([bento_name, ""])
                                        else:
                                            output_data_bento.append([item.strip(), ""])
                                    df_bento_sheet = pd.DataFrame(output_data_bento, columns=['å•†å“äºˆå®šå', 'ãƒ‘ãƒ³ç®±å…¥æ•°'])
                except Exception as e:
                    st.error(f"æ³¨æ–‡å¼å½“ãƒ‡ãƒ¼ã‚¿å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                    traceback.print_exc()

        # 3. è©³ç´°ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæƒ…å ±ã®æŠ½å‡º
        df_client_sheet = None
        if df_paste_sheet is not None:
            with st.spinner("ã€Œã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæŠ½å‡ºã€ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºä¸­..."):
                try:
                    pdf_bytes_io.seek(0)
                    client_data = extract_detailed_client_info_from_pdf(pdf_bytes_io)
                    if client_data:
                        df_client_sheet = export_detailed_client_data_to_dataframe(client_data)
                        st.success(f"ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæƒ…å ± {len(client_data)} ä»¶ã‚’æŠ½å‡ºã—ã¾ã—ãŸ")
                    else:
                        st.warning("ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæƒ…å ±ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                except Exception as e:
                    st.error(f"ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæƒ…å ±æŠ½å‡ºä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                    traceback.print_exc()

        # 4. Excelãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®æ›¸ãè¾¼ã¿ã¨ç”Ÿæˆ
        if df_paste_sheet is not None:
            try:
                # --- template.xlsmã¸ã®æ›¸ãè¾¼ã¿ ---
                with st.spinner("template.xlsm ã«ãƒ‡ãƒ¼ã‚¿ã‚’æ›¸ãè¾¼ã¿ä¸­..."):
                    # è²¼ã‚Šä»˜ã‘ç”¨
                    try:
                        ws_paste = st.session_state.template_wb["è²¼ã‚Šä»˜ã‘ç”¨"]
                        for r_idx, row in df_paste_sheet.iterrows():
                            for c_idx, value in enumerate(row):
                                ws_paste.cell(row=r_idx + 1, column=c_idx + 1, value=value)
                    except KeyError:
                        st.error("template.xlsmã«ã€Œè²¼ã‚Šä»˜ã‘ç”¨ã€ã‚·ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                        st.stop()
                    # æ³¨æ–‡å¼å½“
                    if df_bento_sheet is not None and not df_bento_sheet.empty:
                        try:
                            ws_bento = st.session_state.template_wb["æ³¨æ–‡å¼å½“ã®æŠ½å‡º"]
                            for r in dataframe_to_rows(df_bento_sheet, index=False, header=True):
                                ws_bento.append(r)
                        except KeyError:
                            st.error("template.xlsmã«ã€Œæ³¨æ–‡å¼å½“ã®æŠ½å‡ºã€ã‚·ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                    # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæŠ½å‡º
                    if df_client_sheet is not None and not df_client_sheet.empty:
                        try:
                            ws_client = st.session_state.template_wb["ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæŠ½å‡º"]
                            for r in dataframe_to_rows(df_client_sheet, index=False, header=True):
                                ws_client.append(r)
                        except KeyError:
                            st.error("template.xlsmã«ã€Œã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæŠ½å‡ºã€ã‚·ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                
                # --- nouhinsyo.xlsxã¸ã®æ›¸ãè¾¼ã¿ ---
                with st.spinner("nouhinsyo.xlsx ã«ãƒ‡ãƒ¼ã‚¿ã‚’æ›¸ãè¾¼ã¿ä¸­..."):
                    # âœ…ã€å¤‰æ›´ç‚¹ã€‘nouhinsyo.xlsxç”¨ã®å¼å½“ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆ
                    df_bento_for_nouhin = None
                    if df_bento_sheet is not None and not df_bento_sheet.empty:
                        master_df = st.session_state.master_df
                        # ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã€Œå•†å“äºˆå®šåã€ã‚’ã‚­ãƒ¼ã«ã€Œå•†å“åã€ã‚’å¼•ããŸã‚ã®è¾æ›¸ã‚’ä½œæˆ
                        # `drop_duplicates` ã‚’è¿½åŠ ã—ã¦ã€ã‚­ãƒ¼ã®é‡è¤‡ã‚¨ãƒ©ãƒ¼ã‚’é˜²ã
                        master_map = master_df.drop_duplicates(subset=['å•†å“äºˆå®šå']).set_index('å•†å“äºˆå®šå')['å•†å“å'].to_dict()
                        
                        df_bento_for_nouhin = df_bento_sheet.copy()
                        # mapé–¢æ•°ã‚’ä½¿ã£ã¦ã€Œå•†å“åã€åˆ—ã‚’è¿½åŠ 
                        df_bento_for_nouhin['å•†å“å'] = df_bento_for_nouhin['å•†å“äºˆå®šå'].map(master_map)
                        # åˆ—ã®é †ç•ªã‚’ A:å•†å“äºˆå®šå, B:ãƒ‘ãƒ³ç®±å…¥æ•°, C:å•†å“å ã«ã™ã‚‹
                        df_bento_for_nouhin = df_bento_for_nouhin[['å•†å“äºˆå®šå', 'ãƒ‘ãƒ³ç®±å…¥æ•°', 'å•†å“å']]

                    # è²¼ã‚Šä»˜ã‘ç”¨
                    try:
                        ws_paste_n = st.session_state.nouhinsyo_wb["è²¼ã‚Šä»˜ã‘ç”¨"]
                        for r_idx, row in df_paste_sheet.iterrows():
                            for c_idx, value in enumerate(row):
                                ws_paste_n.cell(row=r_idx + 1, column=c_idx + 1, value=value)
                    except KeyError:
                        st.error("nouhinsyo.xlsxã«ã€Œè²¼ã‚Šä»˜ã‘ç”¨ã€ã‚·ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                    # æ³¨æ–‡å¼å½“
                    if df_bento_for_nouhin is not None and not df_bento_for_nouhin.empty:
                        try:
                            ws_bento_n = st.session_state.nouhinsyo_wb["æ³¨æ–‡å¼å½“ã®æŠ½å‡º"]
                            for r in dataframe_to_rows(df_bento_for_nouhin, index=False, header=True):
                                ws_bento_n.append(r)
                        except KeyError:
                            st.error("nouhinsyo.xlsxã«ã€Œæ³¨æ–‡å¼å½“ã®æŠ½å‡ºã€ã‚·ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                    # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæŠ½å‡º
                    if df_client_sheet is not None and not df_client_sheet.empty:
                        try:
                            ws_client_n = st.session_state.nouhinsyo_wb["ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæŠ½å‡º"]
                            for r in dataframe_to_rows(df_client_sheet, index=False, header=True):
                                ws_client_n.append(r)
                        except KeyError:
                            st.error("nouhinsyo.xlsxã«ã€Œã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæŠ½å‡ºã€ã‚·ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")


                # --- ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒã‚¤ãƒŠãƒªç”Ÿæˆ ---
                # template.xlsm
                output_macro = io.BytesIO()
                st.session_state.template_wb.save(output_macro)
                output_macro.seek(0)
                macro_excel_bytes = output_macro.read()
                
                # nouhinsyo.xlsx
                output_data_only = io.BytesIO()
                st.session_state.nouhinsyo_wb.save(output_data_only)
                output_data_only.seek(0)
                data_only_excel_bytes = output_data_only.read()

                # --- å‡¦ç†å®Œäº†ã¨ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ ---
                st.success("âœ… å…¨ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
                
                col1, col2 = st.columns(2)

                with col1:
                    st.download_button(
                        label="ğŸ“¥ template.xlsmã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=macro_excel_bytes,
                        file_name="template.xlsm", # âœ…ã€å¤‰æ›´ç‚¹ã€‘ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å›ºå®š
                        mime="application/vnd.ms-excel.sheet.macroEnabled.12",
                    )

                with col2:
                    st.download_button(
                        label="ğŸ“¥ nouhinsyo.xlsxã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=data_only_excel_bytes,
                        file_name="nouhinsyo.xlsx", # âœ…ã€å¤‰æ›´ç‚¹ã€‘ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å›ºå®š
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    )

            except Exception as e:
                st.error(f"Excelãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                traceback.print_exc()

# ãƒã‚¹ã‚¿è¨­å®š ãƒšãƒ¼ã‚¸
elif page_selection == "ãƒã‚¹ã‚¿è¨­å®š":
    # (ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯å¤‰æ›´ã‚ã‚Šã¾ã›ã‚“)
    st.markdown('<div class="title">ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿è¨­å®š</div>', unsafe_allow_html=True)
    st.markdown('<div class="subtitle">å•†å“ãƒã‚¹ã‚¿ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦æ›´æ–°ã—ã¾ã™ã€‚</div>', unsafe_allow_html=True)

    master_csv_path = "å•†å“ãƒã‚¹ã‚¿ä¸€è¦§.csv"

    st.markdown("#### æ–°ã—ã„ãƒã‚¹ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    uploaded_master_csv = st.file_uploader(
        "å•†å“ãƒã‚¹ã‚¿ä¸€è¦§.csv ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„",
        type="csv",
        help="ãƒ˜ãƒƒãƒ€ãƒ¼ã«ã¯ 'å•†å“äºˆå®šå', 'ãƒ‘ãƒ³ç®±å…¥æ•°', 'å•†å“å' ã‚’å«ã‚ã¦ãã ã•ã„ã€‚"
    )

    if uploaded_master_csv is not None:
        try:
            new_master_df = None
            encodings = ['utf-8-sig', 'utf-8', 'cp932', 'shift_jis']
            
            for encoding in encodings:
                try:
                    uploaded_master_csv.seek(0)
                    temp_df = pd.read_csv(uploaded_master_csv, encoding=encoding)
                    if all(col in temp_df.columns for col in ['å•†å“äºˆå®šå', 'ãƒ‘ãƒ³ç®±å…¥æ•°', 'å•†å“å']):
                        new_master_df = temp_df
                        st.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ {encoding} ã§èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
                        break
                except UnicodeDecodeError:
                    continue
            
            if new_master_df is not None:
                st.session_state.master_df = new_master_df
                # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
                try:
                    new_master_df.to_csv(master_csv_path, index=False, encoding='utf-8-sig')
                    st.success("ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°ã—ã€ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
                except Exception as e:
                    st.error(f"ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            else:
                st.error("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã«å¿…è¦ãªãƒ˜ãƒƒãƒ€ãƒ¼ï¼ˆ'å•†å“äºˆå®šå', 'ãƒ‘ãƒ³ç®±å…¥æ•°', 'å•†å“å'ï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        except Exception as e:
            st.error(f"ãƒã‚¹ã‚¿CSVã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            traceback.print_exc()

    st.markdown("---")
    st.markdown("#### ç¾åœ¨ã®ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿")
    if 'master_df' in st.session_state and st.session_state.master_df is not None:
        st.dataframe(st.session_state.master_df)
        csv = st.session_state.master_df.to_csv(index=False, encoding='utf-8-sig')
        b64 = base64.b64encode(csv.encode()).decode()
        href = f'<a href="data:file/csv;base64,{b64}" download="å•†å“ãƒã‚¹ã‚¿ä¸€è¦§.csv">ç¾åœ¨ã®ãƒã‚¹ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰</a>'
        st.markdown(href, unsafe_allow_html=True)
    else:
        st.info("ç¾åœ¨ã€èª­ã¿è¾¼ã¾ã‚Œã¦ã„ã‚‹ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
